{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 001 — CatBoost Classification\n",
    "\n",
    "Binary classification with CatBoost, Optuna hyperparameter tuning, threshold\n",
    "optimization, SHAP explanations, and structured metrics output.\n",
    "\n",
    "**Lifecycle stage:** seedling (model-garden)\n",
    "\n",
    "All code is self-contained in this notebook — no external library imports\n",
    "from a shared `src/` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": "# ---------------------------------------------------------------------------\n# Papermill parameters  (this cell is tagged \"parameters\")\n# ---------------------------------------------------------------------------\n\n# Data loading\nfeature_paths: list[str] = []          # local or gs:// URIs; empty → synthetic\ntarget_paths: list[str] = []           # optional separate target files\njoin_key: str | None = None            # key to join features ↔ targets\nfeature_cols: list[str] | None = None  # subset of columns; None → all\ntarget_col: str = \"target\"\npositive_label: str | int | None = None\nentity_id: str | None = \"entity_id\"    # column for group-based train/test split\n\n# Splitting\ntest_size: float = 0.2\nrandom_state: int = 42\nstratify: bool = True\n\n# Optuna\noptuna_n_trials: int = 30\noptuna_timeout_s: int | None = None\noptimize_metric: str = \"f1\"  # \"f1\", \"precision\", \"recall\"\nthreshold_grid: list[float] = [round(x * 0.05, 2) for x in range(1, 20)]\n\n# Outputs\nmetrics_json_path: str = \"outputs/metrics/metrics.json\"\nmodel_output_path: str = \"outputs/models/model.cbm\"\nexecuted_notebook_path: str | None = None\nplots_dir: str = \"outputs/plots\"\n\n# SHAP / feature importance\nshap_sample_size: int = 1000\nenable_shap: bool = True\nenable_feature_importance: bool = True"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ---------------------------------------------------------------------------\n# Imports\n# ---------------------------------------------------------------------------\nimport json\nimport os\nimport warnings\nfrom datetime import datetime, timezone\nfrom pathlib import Path\n\n# Suppress all warnings (keeps notebook output and rendered HTML clean)\nwarnings.filterwarnings(\"ignore\")\nos.environ[\"PYTHONWARNINGS\"] = \"ignore\"\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport polars as pl\nfrom catboost import CatBoostClassifier, Pool\nfrom sklearn.datasets import make_classification\nfrom sklearn.metrics import (\n    auc,\n    average_precision_score,\n    classification_report,\n    confusion_matrix,\n    ConfusionMatrixDisplay,\n    f1_score,\n    precision_recall_curve,\n    precision_score,\n    recall_score,\n    roc_auc_score,\n    roc_curve,\n)\nfrom sklearn.calibration import calibration_curve\nfrom sklearn.model_selection import GroupShuffleSplit, StratifiedKFold, train_test_split\n\n# Ensure output dirs exist\nfor d in [\"outputs/runs\", \"outputs/plots\", \"outputs/models\", \"outputs/metrics\"]:\n    Path(d).mkdir(parents=True, exist_ok=True)\n\nprint(f\"Run started at {datetime.now(timezone.utc).isoformat()}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 — Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ---------------------------------------------------------------------------\n# Data loading helpers (all inline, no external modules)\n# ---------------------------------------------------------------------------\n\ndef _read_file(path: str) -> pl.DataFrame:\n    \"\"\"Read a single parquet or csv file (local or gs://).\"\"\"\n    p = path.strip()\n    if p.endswith(\".parquet\") or p.endswith(\".pq\"):\n        return pl.read_parquet(p)\n    return pl.read_csv(p)\n\n\ndef load_data(\n    feature_paths: list[str],\n    target_paths: list[str],\n    join_key: str | None,\n    feature_cols: list[str] | None,\n    target_col: str,\n    entity_id: str | None,\n) -> pl.DataFrame:\n    \"\"\"Load features (and optional targets), returning a single DataFrame.\"\"\"\n\n    if not feature_paths:\n        # Generate synthetic dataset with entity_id\n        print(\"No feature_paths provided — generating synthetic dataset.\")\n        n_samples = 5_000\n        X, y = make_classification(\n            n_samples=n_samples,\n            n_features=20,\n            n_informative=12,\n            n_redundant=4,\n            n_classes=2,\n            weights=[0.7, 0.3],\n            flip_y=0.03,\n            random_state=random_state,\n        )\n        cols = {f\"feat_{i:02d}\": X[:, i] for i in range(X.shape[1])}\n        cols[target_col] = y\n        # Assign ~3 rows per entity on average so group split is meaningful\n        if entity_id:\n            rng = np.random.RandomState(random_state)\n            n_entities = n_samples // 3\n            cols[entity_id] = rng.randint(0, n_entities, size=n_samples)\n        return pl.DataFrame(cols)\n\n    # Read and concat feature files\n    dfs = [_read_file(p) for p in feature_paths]\n    df = pl.concat(dfs, how=\"vertical_relaxed\")\n\n    # Optionally join separate target files\n    if target_paths:\n        tgt_dfs = [_read_file(p) for p in target_paths]\n        tgt = pl.concat(tgt_dfs, how=\"vertical_relaxed\")\n        if join_key:\n            df = df.join(tgt, on=join_key, how=\"inner\")\n        else:\n            df = pl.concat([df, tgt], how=\"horizontal\")\n\n    # Subset columns if requested\n    if feature_cols is not None:\n        keep = list(set(feature_cols + [target_col] + ([entity_id] if entity_id else [])))\n        df = df.select([c for c in keep if c in df.columns])\n\n    return df\n\n\ndf = load_data(feature_paths, target_paths, join_key, feature_cols, target_col, entity_id)\nprint(f\"Loaded DataFrame: {df.shape[0]:,} rows × {df.shape[1]} cols\")\nif entity_id and entity_id in df.columns:\n    print(f\"Unique entities ({entity_id}): {df[entity_id].n_unique():,}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode positive_label if provided\n",
    "if positive_label is not None:\n",
    "    df = df.with_columns(\n",
    "        (pl.col(target_col).cast(pl.Utf8) == str(positive_label))\n",
    "        .cast(pl.Int8)\n",
    "        .alias(target_col)\n",
    "    )\n",
    "    print(f\"Recoded target: positive_label='{positive_label}' → 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 — EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# Schema & nulls\n",
    "# ---------------------------------------------------------------------------\n",
    "print(\"Schema:\")\n",
    "for name, dtype in zip(df.columns, df.dtypes):\n",
    "    null_ct = df[name].null_count()\n",
    "    print(f\"  {name:30s}  {str(dtype):15s}  nulls={null_ct}\")\n",
    "\n",
    "print(f\"\\nTarget distribution ({target_col}):\")\n",
    "print(df[target_col].value_counts().sort(target_col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# Summary stats for numeric columns\n",
    "# ---------------------------------------------------------------------------\n",
    "numeric_cols = [c for c in df.columns if df[c].dtype in (pl.Float32, pl.Float64, pl.Int8, pl.Int16, pl.Int32, pl.Int64, pl.UInt8, pl.UInt16, pl.UInt32, pl.UInt64)]\n",
    "feature_numeric = [c for c in numeric_cols if c != target_col]\n",
    "\n",
    "if feature_numeric:\n",
    "    print(df.select(feature_numeric).describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# Histograms (first 8 numeric features)\n",
    "# ---------------------------------------------------------------------------\n",
    "plot_cols = feature_numeric[:8]\n",
    "if plot_cols:\n",
    "    n = len(plot_cols)\n",
    "    ncols = min(4, n)\n",
    "    nrows = (n + ncols - 1) // ncols\n",
    "    fig, axes = plt.subplots(nrows, ncols, figsize=(4 * ncols, 3 * nrows))\n",
    "    axes_flat = np.array(axes).flatten() if n > 1 else [axes]\n",
    "    for i, col in enumerate(plot_cols):\n",
    "        axes_flat[i].hist(df[col].drop_nulls().to_numpy(), bins=40, edgecolor=\"white\")\n",
    "        axes_flat[i].set_title(col, fontsize=10)\n",
    "    for j in range(i + 1, len(axes_flat)):\n",
    "        axes_flat[j].set_visible(False)\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(f\"{plots_dir}/eda_histograms.png\", dpi=120)\n",
    "    plt.show()\n",
    "    print(f\"Saved → {plots_dir}/eda_histograms.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# Correlation heatmap (cap at 20 features to keep runtime sane)\n",
    "# ---------------------------------------------------------------------------\n",
    "corr_cols = feature_numeric[:20]\n",
    "if len(corr_cols) >= 2:\n",
    "    corr_df = df.select(corr_cols).to_pandas()\n",
    "    corr = corr_df.corr()\n",
    "    fig, ax = plt.subplots(figsize=(max(6, len(corr_cols) * 0.6), max(5, len(corr_cols) * 0.5)))\n",
    "    im = ax.imshow(corr.values, aspect=\"auto\", cmap=\"RdBu_r\", vmin=-1, vmax=1)\n",
    "    ax.set_xticks(range(len(corr_cols)))\n",
    "    ax.set_yticks(range(len(corr_cols)))\n",
    "    ax.set_xticklabels(corr_cols, rotation=90, fontsize=7)\n",
    "    ax.set_yticklabels(corr_cols, fontsize=7)\n",
    "    fig.colorbar(im, ax=ax, shrink=0.8)\n",
    "    ax.set_title(\"Pairwise Correlation\")\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(f\"{plots_dir}/eda_correlation.png\", dpi=120)\n",
    "    plt.show()\n",
    "    print(f\"Saved → {plots_dir}/eda_correlation.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 — Train / Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ---------------------------------------------------------------------------\n# Split — group-based on entity_id (no entity leaks between train/test)\n# ---------------------------------------------------------------------------\nnon_feature_cols = {target_col}\nif entity_id and entity_id in df.columns:\n    non_feature_cols.add(entity_id)\n\nfeature_names = [c for c in df.columns if c not in non_feature_cols]\nX_all = df.select(feature_names).to_numpy()\ny_all = df[target_col].to_numpy().astype(int)\n\nif entity_id and entity_id in df.columns:\n    groups = df[entity_id].to_numpy()\n    gss = GroupShuffleSplit(n_splits=1, test_size=test_size, random_state=random_state)\n    train_idx, test_idx = next(gss.split(X_all, y_all, groups=groups))\n    X_train, X_test = X_all[train_idx], X_all[test_idx]\n    y_train, y_test = y_all[train_idx], y_all[test_idx]\n    n_train_entities = len(set(groups[train_idx]))\n    n_test_entities = len(set(groups[test_idx]))\n    assert len(set(groups[train_idx]) & set(groups[test_idx])) == 0, \"Entity leak detected!\"\n    print(f\"Group split on '{entity_id}': {n_train_entities:,} train entities, {n_test_entities:,} test entities\")\nelse:\n    split_kw = dict(test_size=test_size, random_state=random_state)\n    if stratify:\n        split_kw[\"stratify\"] = y_all\n    X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, **split_kw)\n    print(\"Random split (no entity_id column found)\")\n\n# Compute scale_pos_weight from training set class imbalance\nn_neg = int((y_train == 0).sum())\nn_pos = int((y_train == 1).sum())\nscale_pos_weight = n_neg / n_pos\n\nprint(f\"Train: {X_train.shape[0]:,}  |  Test: {X_test.shape[0]:,}\")\nprint(f\"Train target rate: {y_train.mean():.3f}  |  Test target rate: {y_test.mean():.3f}\")\nprint(f\"Class balance — neg: {n_neg:,}  pos: {n_pos:,}  → scale_pos_weight: {scale_pos_weight:.4f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 — Optuna Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ---------------------------------------------------------------------------\n# Optuna objective\n# ---------------------------------------------------------------------------\nimport optuna\n\noptuna.logging.set_verbosity(optuna.logging.WARNING)\n\n_METRIC_FN = {\n    \"f1\": f1_score,\n    \"precision\": precision_score,\n    \"recall\": recall_score,\n}\n\n\ndef objective(trial: optuna.Trial) -> float:\n    params = {\n        \"iterations\": trial.suggest_int(\"iterations\", 200, 1500),\n        \"depth\": trial.suggest_int(\"depth\", 3, 10),\n        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True),\n        \"l2_leaf_reg\": trial.suggest_float(\"l2_leaf_reg\", 1e-2, 10.0, log=True),\n        \"border_count\": trial.suggest_int(\"border_count\", 32, 255),\n        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n        \"random_strength\": trial.suggest_float(\"random_strength\", 1e-3, 10.0, log=True),\n        \"scale_pos_weight\": scale_pos_weight,\n    }\n\n    metric_fn = _METRIC_FN[optimize_metric]\n    skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=random_state)\n    fold_scores = []\n\n    for train_idx, val_idx in skf.split(X_train, y_train):\n        xtr, xvl = X_train[train_idx], X_train[val_idx]\n        ytr, yvl = y_train[train_idx], y_train[val_idx]\n\n        model = CatBoostClassifier(\n            **params,\n            eval_metric=\"Logloss\",\n            random_seed=random_state,\n            verbose=0,\n            early_stopping_rounds=50,\n        )\n        model.fit(xtr, ytr, eval_set=(xvl, yvl), verbose=0)\n\n        proba = model.predict_proba(xvl)[:, 1]\n        best_score = max(\n            metric_fn(yvl, (proba >= t).astype(int), zero_division=0)\n            for t in threshold_grid\n        )\n        fold_scores.append(best_score)\n\n    return float(np.mean(fold_scores))\n\n\nstudy = optuna.create_study(direction=\"maximize\")\nstudy.optimize(objective, n_trials=optuna_n_trials, timeout=optuna_timeout_s)\n\nbest_params = study.best_params\nprint(f\"Best CV {optimize_metric}: {study.best_value:.4f}\")\nprint(f\"Best params: {json.dumps(best_params, indent=2)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 — Train Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ---------------------------------------------------------------------------\n# Final model on full train set\n# ---------------------------------------------------------------------------\nfinal_model = CatBoostClassifier(\n    **best_params,\n    scale_pos_weight=scale_pos_weight,\n    eval_metric=\"Logloss\",\n    random_seed=random_state,\n    verbose=100,\n)\nfinal_model.fit(X_train, y_train)\n\n# Save model\nPath(model_output_path).parent.mkdir(parents=True, exist_ok=True)\nfinal_model.save_model(model_output_path)\nprint(f\"\\nModel saved → {model_output_path}\")\nprint(f\"scale_pos_weight used: {scale_pos_weight:.4f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 — Evaluation & Threshold Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# Per-threshold metrics on test set\n",
    "# ---------------------------------------------------------------------------\n",
    "y_proba = final_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "threshold_rows = []\n",
    "for t in threshold_grid:\n",
    "    y_pred_t = (y_proba >= t).astype(int)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred_t, labels=[0, 1]).ravel()\n",
    "    threshold_rows.append({\n",
    "        \"threshold\": round(t, 4),\n",
    "        \"precision\": round(precision_score(y_test, y_pred_t, zero_division=0), 4),\n",
    "        \"recall\": round(recall_score(y_test, y_pred_t, zero_division=0), 4),\n",
    "        \"f1\": round(f1_score(y_test, y_pred_t, zero_division=0), 4),\n",
    "        \"tp\": int(tp),\n",
    "        \"fp\": int(fp),\n",
    "        \"tn\": int(tn),\n",
    "        \"fn\": int(fn),\n",
    "    })\n",
    "\n",
    "threshold_df = pl.DataFrame(threshold_rows)\n",
    "print(threshold_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ---------------------------------------------------------------------------\n# Best threshold per metric\n# ---------------------------------------------------------------------------\nbest_f1_row = threshold_df.sort(\"f1\", descending=True).row(0, named=True)\nbest_precision_row = threshold_df.sort(\"precision\", descending=True).row(0, named=True)\nbest_recall_row = threshold_df.sort(\"recall\", descending=True).row(0, named=True)\n\n# Primary best threshold is based on optimize_metric\nbest_row = {\"f1\": best_f1_row, \"precision\": best_precision_row, \"recall\": best_recall_row}[optimize_metric]\nbest_threshold = best_row[\"threshold\"]\n\nprint(f\"Best threshold (by {optimize_metric}): {best_threshold}\")\nprint(f\"  precision={best_row['precision']:.4f}  recall={best_row['recall']:.4f}  f1={best_row['f1']:.4f}\")\nprint()\nprint(f\"Best F1 threshold:        {best_f1_row['threshold']}  (F1={best_f1_row['f1']:.4f})\")\nprint(f\"Best Precision threshold:  {best_precision_row['threshold']}  (Precision={best_precision_row['precision']:.4f})\")\nprint(f\"Best Recall threshold:     {best_recall_row['threshold']}  (Recall={best_recall_row['recall']:.4f})\")\n\n# AUC scores (threshold-independent)\nroc_auc = roc_auc_score(y_test, y_proba)\npr_auc = average_precision_score(y_test, y_proba)\nprint(f\"\\nROC-AUC: {roc_auc:.4f}\")\nprint(f\"PR-AUC (Average Precision): {pr_auc:.4f}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ---------------------------------------------------------------------------\n# Individual metric vs threshold plots\n# ---------------------------------------------------------------------------\nthresholds = threshold_df[\"threshold\"].to_list()\n\nmetric_configs = [\n    (\"f1\", \"F1 Score\", best_f1_row, \"#2196F3\"),\n    (\"precision\", \"Precision\", best_precision_row, \"#4CAF50\"),\n    (\"recall\", \"Recall\", best_recall_row, \"#FF9800\"),\n]\n\nfig, axes = plt.subplots(1, 3, figsize=(18, 5))\n\nfor ax, (metric_key, metric_label, best_metric_row, color) in zip(axes, metric_configs):\n    values = threshold_df[metric_key].to_list()\n    best_thr = best_metric_row[\"threshold\"]\n    best_val = best_metric_row[metric_key]\n\n    ax.plot(thresholds, values, marker=\".\", color=color, linewidth=2)\n    ax.axvline(best_thr, color=\"grey\", linestyle=\"--\", alpha=0.7)\n    ax.plot(best_thr, best_val, \"r*\", markersize=15, zorder=5)\n    ax.annotate(\n        f\"  thr={best_thr}\\n  {metric_label}={best_val:.4f}\",\n        xy=(best_thr, best_val),\n        fontsize=8,\n        color=\"red\",\n    )\n    ax.set_xlabel(\"Threshold\")\n    ax.set_ylabel(metric_label)\n    ax.set_title(f\"{metric_label} vs Threshold\")\n    ax.grid(True, alpha=0.3)\n    ax.set_ylim(-0.05, 1.05)\n\nfig.tight_layout()\nfig.savefig(f\"{plots_dir}/threshold_individual.png\", dpi=120)\nplt.show()\nprint(f\"Saved → {plots_dir}/threshold_individual.png\")"
  },
  {
   "cell_type": "code",
   "source": "# ---------------------------------------------------------------------------\n# Combined overlay: Precision / Recall / F1 vs Threshold\n# ---------------------------------------------------------------------------\nfig, ax = plt.subplots(figsize=(9, 5))\nax.plot(thresholds, threshold_df[\"precision\"].to_list(), label=\"Precision\", marker=\".\", color=\"#4CAF50\")\nax.plot(thresholds, threshold_df[\"recall\"].to_list(), label=\"Recall\", marker=\".\", color=\"#FF9800\")\nax.plot(thresholds, threshold_df[\"f1\"].to_list(), label=\"F1\", marker=\".\", linewidth=2, color=\"#2196F3\")\nax.axvline(best_threshold, color=\"red\", linestyle=\"--\", alpha=0.7, label=f\"Best thr ({optimize_metric})={best_threshold}\")\nax.set_xlabel(\"Threshold\")\nax.set_ylabel(\"Score\")\nax.set_title(\"Precision / Recall / F1 vs Threshold\")\nax.legend()\nax.grid(True, alpha=0.3)\nax.set_ylim(-0.05, 1.05)\nfig.tight_layout()\nfig.savefig(f\"{plots_dir}/threshold_overlay.png\", dpi=120)\nplt.show()\nprint(f\"Saved → {plots_dir}/threshold_overlay.png\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ---------------------------------------------------------------------------\n# ROC Curve\n# ---------------------------------------------------------------------------\nfpr, tpr, roc_thresholds = roc_curve(y_test, y_proba)\n\nfig, ax = plt.subplots(figsize=(7, 6))\nax.plot(fpr, tpr, linewidth=2, label=f\"ROC (AUC = {roc_auc:.4f})\")\nax.plot([0, 1], [0, 1], \"k--\", alpha=0.4, label=\"Random classifier\")\n\n# Mark the operating point at best_threshold\ny_pred_bt = (y_proba >= best_threshold).astype(int)\ntn_bt, fp_bt, fn_bt, tp_bt = confusion_matrix(y_test, y_pred_bt, labels=[0, 1]).ravel()\nfpr_bt = fp_bt / (fp_bt + tn_bt)\ntpr_bt = tp_bt / (tp_bt + fn_bt)\nax.plot(fpr_bt, tpr_bt, \"r*\", markersize=15, zorder=5, label=f\"Best thr={best_threshold}\")\n\nax.set_xlabel(\"False Positive Rate\")\nax.set_ylabel(\"True Positive Rate\")\nax.set_title(\"ROC Curve\")\nax.legend(loc=\"lower right\")\nax.grid(True, alpha=0.3)\nax.set_xlim(-0.02, 1.02)\nax.set_ylim(-0.02, 1.02)\nfig.tight_layout()\nfig.savefig(f\"{plots_dir}/roc_curve.png\", dpi=120)\nplt.show()\nprint(f\"Saved → {plots_dir}/roc_curve.png\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ---------------------------------------------------------------------------\n# Precision-Recall Curve\n# ---------------------------------------------------------------------------\npr_precision, pr_recall, pr_thresholds = precision_recall_curve(y_test, y_proba)\n\nfig, ax = plt.subplots(figsize=(7, 6))\nax.plot(pr_recall, pr_precision, linewidth=2, label=f\"PR curve (AP = {pr_auc:.4f})\")\n\n# Baseline: prevalence of positive class\nprevalence = y_test.mean()\nax.axhline(prevalence, color=\"k\", linestyle=\"--\", alpha=0.4, label=f\"Baseline (prevalence={prevalence:.3f})\")\n\n# Mark the operating point at best_threshold\nprec_bt = best_row[\"precision\"]\nrec_bt = best_row[\"recall\"]\nax.plot(rec_bt, prec_bt, \"r*\", markersize=15, zorder=5, label=f\"Best thr={best_threshold}\")\n\nax.set_xlabel(\"Recall\")\nax.set_ylabel(\"Precision\")\nax.set_title(\"Precision-Recall Curve\")\nax.legend(loc=\"upper right\")\nax.grid(True, alpha=0.3)\nax.set_xlim(-0.02, 1.02)\nax.set_ylim(-0.02, 1.05)\nfig.tight_layout()\nfig.savefig(f\"{plots_dir}/pr_curve.png\", dpi=120)\nplt.show()\nprint(f\"Saved → {plots_dir}/pr_curve.png\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ---------------------------------------------------------------------------\n# Predicted Probability Distribution by Class\n# ---------------------------------------------------------------------------\nfig, ax = plt.subplots(figsize=(8, 5))\n\nmask_neg = y_test == 0\nmask_pos = y_test == 1\nax.hist(y_proba[mask_neg], bins=50, alpha=0.6, label=\"Class 0 (negative)\", color=\"#2196F3\", edgecolor=\"white\")\nax.hist(y_proba[mask_pos], bins=50, alpha=0.6, label=\"Class 1 (positive)\", color=\"#FF5722\", edgecolor=\"white\")\nax.axvline(best_threshold, color=\"red\", linestyle=\"--\", linewidth=2, label=f\"Best thr={best_threshold}\")\nax.set_xlabel(\"Predicted Probability\")\nax.set_ylabel(\"Count\")\nax.set_title(\"Predicted Probability Distribution by Class\")\nax.legend()\nax.grid(True, alpha=0.3)\nfig.tight_layout()\nfig.savefig(f\"{plots_dir}/probability_distribution.png\", dpi=120)\nplt.show()\nprint(f\"Saved → {plots_dir}/probability_distribution.png\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ---------------------------------------------------------------------------\n# Calibration Curve\n# ---------------------------------------------------------------------------\nprob_true, prob_pred = calibration_curve(y_test, y_proba, n_bins=10, strategy=\"uniform\")\n\nfig, ax = plt.subplots(figsize=(7, 6))\nax.plot(prob_pred, prob_true, \"s-\", linewidth=2, label=\"Model\")\nax.plot([0, 1], [0, 1], \"k--\", alpha=0.4, label=\"Perfectly calibrated\")\nax.set_xlabel(\"Mean Predicted Probability\")\nax.set_ylabel(\"Fraction of Positives\")\nax.set_title(\"Calibration Curve (Reliability Diagram)\")\nax.legend(loc=\"lower right\")\nax.grid(True, alpha=0.3)\nax.set_xlim(-0.02, 1.02)\nax.set_ylim(-0.02, 1.02)\nfig.tight_layout()\nfig.savefig(f\"{plots_dir}/calibration_curve.png\", dpi=120)\nplt.show()\nprint(f\"Saved → {plots_dir}/calibration_curve.png\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ---------------------------------------------------------------------------\n# Cumulative Gains & Lift Chart\n# ---------------------------------------------------------------------------\n# Sort by predicted probability descending\norder = np.argsort(-y_proba)\ny_sorted = y_test[order]\nn = len(y_sorted)\nn_pos = y_sorted.sum()\n\n# Cumulative gains: fraction of positives captured vs fraction of population examined\ncum_pos = np.cumsum(y_sorted)\npct_population = np.arange(1, n + 1) / n\npct_captured = cum_pos / n_pos\n\n# Lift: ratio of cumulative gains to random baseline\nlift = pct_captured / pct_population\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Cumulative gains\nax = axes[0]\nax.plot(pct_population, pct_captured, linewidth=2, label=\"Model\")\nax.plot([0, 1], [0, 1], \"k--\", alpha=0.4, label=\"Random\")\nax.plot([0, n_pos / n, 1], [0, 1, 1], \"g--\", alpha=0.4, label=\"Perfect\")\nax.set_xlabel(\"Fraction of Population\")\nax.set_ylabel(\"Fraction of Positives Captured\")\nax.set_title(\"Cumulative Gains Chart\")\nax.legend(loc=\"lower right\")\nax.grid(True, alpha=0.3)\n\n# Lift\nax = axes[1]\n# Downsample for smooth plotting\nstep = max(1, n // 200)\nax.plot(pct_population[::step], lift[::step], linewidth=2, color=\"#9C27B0\")\nax.axhline(1.0, color=\"k\", linestyle=\"--\", alpha=0.4, label=\"Baseline (lift=1)\")\nax.set_xlabel(\"Fraction of Population\")\nax.set_ylabel(\"Lift\")\nax.set_title(\"Lift Chart\")\nax.legend()\nax.grid(True, alpha=0.3)\n\nfig.tight_layout()\nfig.savefig(f\"{plots_dir}/cumulative_gains_lift.png\", dpi=120)\nplt.show()\nprint(f\"Saved → {plots_dir}/cumulative_gains_lift.png\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# Confusion matrix at best threshold\n",
    "# ---------------------------------------------------------------------------\n",
    "y_pred_final = (y_proba >= best_threshold).astype(int)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 4))\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, y_pred_final, ax=ax, cmap=\"Blues\")\n",
    "ax.set_title(f\"Confusion Matrix (threshold={best_threshold})\")\n",
    "fig.tight_layout()\n",
    "fig.savefig(f\"{plots_dir}/confusion_matrix.png\", dpi=120)\n",
    "plt.show()\n",
    "print(f\"Saved → {plots_dir}/confusion_matrix.png\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_final))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 — Metrics JSON Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ---------------------------------------------------------------------------\n# Write structured metrics JSON\n# ---------------------------------------------------------------------------\nmetrics_output = {\n    \"run_metadata\": {\n        \"timestamp\": datetime.now(timezone.utc).isoformat(),\n        \"target_col\": target_col,\n        \"entity_id\": entity_id,\n        \"test_size\": test_size,\n        \"random_state\": random_state,\n        \"stratify\": stratify,\n        \"scale_pos_weight\": round(scale_pos_weight, 4),\n        \"optuna_n_trials\": optuna_n_trials,\n        \"optimize_metric\": optimize_metric,\n        \"n_train\": int(X_train.shape[0]),\n        \"n_test\": int(X_test.shape[0]),\n        \"n_features\": int(X_train.shape[1]),\n        \"feature_names\": feature_names,\n        \"best_optuna_cv_score\": round(study.best_value, 4),\n        \"best_params\": best_params,\n        \"best_threshold\": best_threshold,\n        \"model_output_path\": model_output_path,\n        \"roc_auc\": round(roc_auc, 4),\n        \"pr_auc\": round(pr_auc, 4),\n    },\n    \"best_thresholds_per_metric\": {\n        \"f1\": {\"threshold\": best_f1_row[\"threshold\"], \"value\": best_f1_row[\"f1\"]},\n        \"precision\": {\"threshold\": best_precision_row[\"threshold\"], \"value\": best_precision_row[\"precision\"]},\n        \"recall\": {\"threshold\": best_recall_row[\"threshold\"], \"value\": best_recall_row[\"recall\"]},\n    },\n    \"per_threshold\": threshold_rows,\n    \"best_threshold_metrics\": best_row,\n}\n\nPath(metrics_json_path).parent.mkdir(parents=True, exist_ok=True)\nwith open(metrics_json_path, \"w\") as f:\n    json.dump(metrics_output, f, indent=2, default=str)\n\nprint(f\"Metrics JSON saved → {metrics_json_path}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8 — Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# CatBoost feature importance\n",
    "# ---------------------------------------------------------------------------\n",
    "if enable_feature_importance:\n",
    "    importances = final_model.get_feature_importance()\n",
    "    imp_df = (\n",
    "        pl.DataFrame({\"feature\": feature_names, \"importance\": importances})\n",
    "        .sort(\"importance\", descending=True)\n",
    "    )\n",
    "    top_n = min(20, len(imp_df))\n",
    "    top = imp_df.head(top_n)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, max(4, top_n * 0.35)))\n",
    "    ax.barh(top[\"feature\"].to_list()[::-1], top[\"importance\"].to_list()[::-1])\n",
    "    ax.set_xlabel(\"Importance\")\n",
    "    ax.set_title(f\"Top {top_n} Feature Importances\")\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(f\"{plots_dir}/feature_importance.png\", dpi=120)\n",
    "    plt.show()\n",
    "    print(f\"Saved → {plots_dir}/feature_importance.png\")\n",
    "else:\n",
    "    print(\"Feature importance disabled.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9 — SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# SHAP explanations\n",
    "# ---------------------------------------------------------------------------\n",
    "if enable_shap:\n",
    "    import shap\n",
    "\n",
    "    n_sample = min(shap_sample_size, X_test.shape[0])\n",
    "    rng = np.random.RandomState(random_state)\n",
    "    idx = rng.choice(X_test.shape[0], size=n_sample, replace=False)\n",
    "    X_shap = X_test[idx]\n",
    "\n",
    "    explainer = shap.TreeExplainer(final_model)\n",
    "    shap_values = explainer.shap_values(X_shap)\n",
    "\n",
    "    # Summary bar plot\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    shap.summary_plot(\n",
    "        shap_values, X_shap,\n",
    "        feature_names=feature_names,\n",
    "        plot_type=\"bar\",\n",
    "        show=False,\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{plots_dir}/shap_bar.png\", dpi=120, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    print(f\"Saved → {plots_dir}/shap_bar.png\")\n",
    "\n",
    "    # Summary dot plot\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    shap.summary_plot(\n",
    "        shap_values, X_shap,\n",
    "        feature_names=feature_names,\n",
    "        show=False,\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{plots_dir}/shap_summary.png\", dpi=120, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    print(f\"Saved → {plots_dir}/shap_summary.png\")\n",
    "else:\n",
    "    print(\"SHAP disabled.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10 — Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ---------------------------------------------------------------------------\n# Final summary\n# ---------------------------------------------------------------------------\nprint(\"=\" * 60)\nprint(\"RUN COMPLETE\")\nprint(\"=\" * 60)\nprint(f\"  Model saved to:       {model_output_path}\")\nprint(f\"  Metrics JSON:         {metrics_json_path}\")\nprint(f\"  Plots directory:      {plots_dir}\")\nif executed_notebook_path:\n    print(f\"  Executed notebook:    {executed_notebook_path}\")\nprint(f\"  Best threshold:       {best_threshold}\")\nprint(f\"  F1 at best threshold: {best_row['f1']:.4f}\")\nprint(f\"  ROC-AUC:              {roc_auc:.4f}\")\nprint(f\"  PR-AUC:               {pr_auc:.4f}\")\nprint(\"=\" * 60)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}