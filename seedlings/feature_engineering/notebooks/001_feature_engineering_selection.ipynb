{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 001 \u2014 Feature Engineering & Selection\n",
    "\n",
    "Comprehensive, plot-heavy feature engineering and selection pipeline.\n",
    "Given an input parquet dataset and a declared target column, this notebook\n",
    "produces curated feature recommendations, quality metrics, engineered\n",
    "features, and a full suite of diagnostic plots.\n",
    "\n",
    "**Lifecycle stage:** seedling (model-garden)\n",
    "\n",
    "All code is self-contained in this notebook \u2014 no external library imports\n",
    "from a shared `src/` package.\n",
    "\n",
    "## Feature selection philosophy\n",
    "\n",
    "1. **Relevance** \u2014 keep features that carry signal about the target.\n",
    "2. **Stability** \u2014 prefer features whose importance is consistent across\n",
    "   CV folds and time periods.\n",
    "3. **Leakage avoidance** \u2014 flag and remove features that would not be\n",
    "   available at prediction time or that encode the target directly.\n",
    "4. **Parsimony** \u2014 fewer, stronger features beat a large, noisy set.\n",
    "\n",
    "## Outputs produced\n",
    "\n",
    "| Artifact | Path |\n",
    "|---|---|\n",
    "| Metrics JSON | `outputs/metrics/feature_report.json` |\n",
    "| Plots | `outputs/plots/*.png` |\n",
    "| Transformed features parquet | `outputs/features/features.parquet` |\n",
    "| Executed notebook | `outputs/runs/<timestamp>_executed.ipynb` |\n",
    "\n",
    "## Running with papermill\n",
    "\n",
    "```bash\n",
    "uv run papermill notebooks/001_feature_engineering_selection.ipynb out.ipynb \\\n",
    "    -p target_col \"target\" \\\n",
    "    -p task_type \"binary\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# Papermill parameters  (this cell is tagged \"parameters\")\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "# Data + schema\n",
    "input_parquet_paths: list[str] = []       # local or gs:// URIs; empty -> synthetic\n",
    "output_dir: str = \"outputs\"\n",
    "plots_dir: str = \"outputs/plots\"\n",
    "metrics_json_path: str = \"outputs/metrics/feature_report.json\"\n",
    "output_features_parquet_path: str | None = \"outputs/features/features.parquet\"\n",
    "executed_notebook_path: str | None = None\n",
    "target_col: str = \"target\"\n",
    "id_cols: list[str] = []                   # entity IDs / keys\n",
    "time_col: str | None = None              # datetime column for time-aware features\n",
    "group_cols: list[str] = []               # entity grouping keys for aggregation\n",
    "categorical_cols: list[str] | None = None  # None -> infer via dtype / unique count\n",
    "numeric_cols: list[str] | None = None      # None -> infer\n",
    "text_cols: list[str] | None = None         # optional lightweight text features\n",
    "drop_cols: list[str] = []                  # columns to always drop\n",
    "max_rows_for_eda: int = 200_000            # sample for heavy plots\n",
    "sample_seed: int = 42\n",
    "\n",
    "# Missingness + outliers\n",
    "missingness_drop_threshold: float = 0.98\n",
    "high_cardinality_threshold: int = 500\n",
    "rare_category_min_count: int = 20\n",
    "winsorize_limits: list[float] = [0.01, 0.99]\n",
    "enable_outlier_clipping: bool = True\n",
    "\n",
    "# Feature generation toggles\n",
    "enable_interactions: bool = True\n",
    "enable_polynomial: bool = False\n",
    "enable_group_aggregations: bool = True\n",
    "enable_time_features: bool = True\n",
    "enable_target_encoding: bool = False\n",
    "enable_mutual_info: bool = True\n",
    "enable_permutation_importance: bool = True\n",
    "enable_shap: bool = False\n",
    "enable_stability_checks: bool = True\n",
    "\n",
    "# Selection + evaluation\n",
    "task_type: str = \"binary\"                  # binary | multiclass | regression\n",
    "test_size: float = 0.2\n",
    "random_state: int = 42\n",
    "stratify: bool = True\n",
    "baseline_model: str = \"logreg\"             # logreg | catboost_if_available\n",
    "cv_folds: int = 3\n",
    "scoring_metric: str = \"f1\"                 # f1 | f1_macro | rmse"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# Imports & setup\n",
    "# ---------------------------------------------------------------------------\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import warnings\n",
    "from datetime import date, datetime, timedelta, timezone\n",
    "from pathlib import Path\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ[\"PYTHONWARNINGS\"] = \"ignore\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "import polars as pl\n",
    "\n",
    "# sklearn / scipy are used ONLY at model / statistical-test boundaries.\n",
    "# Every data-wrangling step uses Polars.\n",
    "import numpy as np  # only for sklearn interop and matplotlib arrays\n",
    "from scipy import stats as sp_stats\n",
    "from sklearn.feature_selection import (\n",
    "    chi2,\n",
    "    f_classif,\n",
    "    f_regression,\n",
    "    mutual_info_classif,\n",
    "    mutual_info_regression,\n",
    ")\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.linear_model import LogisticRegression, Ridge\n",
    "from sklearn.metrics import (\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    mean_squared_error,\n",
    ")\n",
    "from sklearn.model_selection import (\n",
    "    StratifiedKFold,\n",
    "    KFold,\n",
    "    train_test_split,\n",
    ")\n",
    "from sklearn.preprocessing import OrdinalEncoder, StandardScaler\n",
    "\n",
    "# Ensure output dirs exist\n",
    "for d in [\"outputs/runs\", \"outputs/plots\", \"outputs/metrics\", \"outputs/features\"]:\n",
    "    Path(d).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "RUN_TS = datetime.now(timezone.utc).isoformat()\n",
    "print(f\"Run started at {RUN_TS}\")\n",
    "\n",
    "# Matplotlib defaults\n",
    "plt.rcParams.update({\n",
    "    \"figure.figsize\": (10, 5),\n",
    "    \"figure.dpi\": 120,\n",
    "    \"axes.titlesize\": 13,\n",
    "    \"axes.labelsize\": 11,\n",
    "    \"xtick.labelsize\": 9,\n",
    "    \"ytick.labelsize\": 9,\n",
    "    \"figure.facecolor\": \"white\",\n",
    "})"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# Helper utilities (Polars-first)\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def save_plot(fig, name: str) -> str:\n",
    "    path = f\"{plots_dir}/{name}\"\n",
    "    fig.savefig(path, bbox_inches=\"tight\", facecolor=\"white\")\n",
    "    plt.show()\n",
    "    plt.close(fig)\n",
    "    print(f\"  -> saved {path}\")\n",
    "    return path\n",
    "\n",
    "\n",
    "def safe_sample(df: pl.DataFrame, n: int, seed: int = 42) -> pl.DataFrame:\n",
    "    if len(df) <= n:\n",
    "        return df\n",
    "    return df.sample(n=n, seed=seed)\n",
    "\n",
    "\n",
    "def polars_col_to_list(df: pl.DataFrame, col: str) -> list:\n",
    "    \"\"\"Extract a column as a Python list (for matplotlib).\"\"\"\n",
    "    return df[col].to_list()\n",
    "\n",
    "\n",
    "def polars_to_numpy_for_sklearn(\n",
    "    df: pl.DataFrame,\n",
    "    feature_cols: list[str],\n",
    "    target: str,\n",
    "    cat_cols: list[str] | None = None,\n",
    ") -> tuple:\n",
    "    \"\"\"Convert Polars DataFrame to numpy arrays ONLY for sklearn.\n",
    "\n",
    "    This is the single boundary where we leave Polars. All data prep\n",
    "    (fill_null, casting, encoding) happens in Polars first.\n",
    "    \"\"\"\n",
    "    # Fill nulls and cast in Polars before conversion\n",
    "    exprs = []\n",
    "    for c in feature_cols:\n",
    "        dt = df[c].dtype\n",
    "        if dt in (pl.Utf8, pl.String, pl.Categorical):\n",
    "            exprs.append(pl.col(c).fill_null(\"__MISSING__\"))\n",
    "        elif dt.is_numeric():\n",
    "            exprs.append(pl.col(c).fill_null(pl.col(c).median()))\n",
    "        else:\n",
    "            exprs.append(pl.col(c).fill_null(pl.lit(0)))\n",
    "    sub = df.select(exprs + [pl.col(target)])\n",
    "\n",
    "    # Ordinal-encode string columns in Polars\n",
    "    actual_cats = [c for c in (cat_cols or []) if c in feature_cols and sub[c].dtype in (pl.Utf8, pl.String)]\n",
    "    if actual_cats:\n",
    "        for c in actual_cats:\n",
    "            # Map each unique value to an integer\n",
    "            uniques = sub[c].unique().sort()\n",
    "            mapping = {v: i for i, v in enumerate(uniques.to_list())}\n",
    "            sub = sub.with_columns(\n",
    "                pl.col(c).replace_strict(mapping, default=-1).cast(pl.Float64).alias(c)\n",
    "            )\n",
    "\n",
    "    # All columns should now be numeric \u2014 cast to Float64 and convert\n",
    "    num_exprs = [pl.col(c).cast(pl.Float64) for c in feature_cols]\n",
    "    X = sub.select(num_exprs).to_numpy()\n",
    "    y = sub[target].to_numpy()\n",
    "    # Replace any remaining NaN/inf (safety net)\n",
    "    X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "# Tracking containers\n",
    "report: dict = {\n",
    "    \"run_metadata\": {\n",
    "        \"timestamp\": RUN_TS,\n",
    "        \"task_type\": task_type,\n",
    "        \"target_col\": target_col,\n",
    "    },\n",
    "    \"column_audit\": [],\n",
    "    \"dropped_features\": [],\n",
    "    \"univariate_scores\": {},\n",
    "    \"consensus_ranking\": [],\n",
    "    \"model_based_importance\": {},\n",
    "    \"selection_frequency\": {},\n",
    "    \"top_k_performance\": [],\n",
    "    \"recommended_features\": [],\n",
    "    \"engineered_features_created\": [],\n",
    "    \"high_risk_features\": [],\n",
    "}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## B \u2014 Load Data (Polars-first)\n",
    "\n",
    "Load parquet file(s) from `input_parquet_paths`. Supports local paths and\n",
    "`gs://` URIs via `gcsfs`. If no paths are provided, generate a synthetic\n",
    "dataset for demonstration."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# B \u2014 Data Loading (pure Polars)\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def load_parquet_polars(paths: list[str] | str) -> pl.DataFrame:\n",
    "    if isinstance(paths, str):\n",
    "        paths = [paths]\n",
    "    frames = []\n",
    "    for p in paths:\n",
    "        if p.startswith(\"gs://\"):\n",
    "            import gcsfs\n",
    "            fs = gcsfs.GCSFileSystem()\n",
    "            with fs.open(p, \"rb\") as f:\n",
    "                frames.append(pl.read_parquet(f))\n",
    "        else:\n",
    "            frames.append(pl.read_parquet(p))\n",
    "    return pl.concat(frames) if len(frames) > 1 else frames[0]\n",
    "\n",
    "\n",
    "def generate_synthetic(task: str, n: int = 5000, seed: int = 42) -> pl.DataFrame:\n",
    "    \"\"\"Create a synthetic dataset entirely in Polars.\"\"\"\n",
    "    random.seed(seed)\n",
    "\n",
    "    # Build numeric features via sklearn (returns numpy), wrap once\n",
    "    from sklearn.datasets import make_classification, make_regression\n",
    "    if task == \"regression\":\n",
    "        X, y = make_regression(n_samples=n, n_features=20, n_informative=10,\n",
    "                               noise=10.0, random_state=seed)\n",
    "    else:\n",
    "        n_classes = 2 if task == \"binary\" else 4\n",
    "        X, y = make_classification(n_samples=n, n_features=20, n_informative=10,\n",
    "                                   n_redundant=3, n_classes=n_classes,\n",
    "                                   weights=[0.7, 0.3] if task == \"binary\" else None,\n",
    "                                   random_state=seed)\n",
    "\n",
    "    # Wrap numeric features into a Polars DataFrame in one shot\n",
    "    df = pl.DataFrame(\n",
    "        {f\"num_{i}\": pl.Series(X[:, i]) for i in range(X.shape[1])}\n",
    "    ).with_columns(\n",
    "        pl.Series(\"target\", y.astype(float) if task == \"regression\" else y),\n",
    "    )\n",
    "\n",
    "    # Categorical columns \u2014 built as plain Python lists\n",
    "    df = df.with_columns([\n",
    "        pl.Series(\"cat_region\",  [random.choice([\"US\", \"EU\", \"APAC\", \"LATAM\", \"MEA\"]) for _ in range(n)]),\n",
    "        pl.Series(\"cat_channel\", [random.choice([\"web\", \"mobile\", \"api\", \"partner\"]) for _ in range(n)]),\n",
    "        pl.Series(\"cat_tier\",    [random.choice([\"free\", \"basic\", \"premium\"]) for _ in range(n)]),\n",
    "    ])\n",
    "\n",
    "    # Rare-category column (v0 is dominant)\n",
    "    rare_choices = [f\"v{i}\" for i in range(50)]\n",
    "    rare_weights = [0.5] + [0.5 / 49] * 49\n",
    "    df = df.with_columns(\n",
    "        pl.Series(\"cat_rare\", random.choices(rare_choices, weights=rare_weights, k=n)),\n",
    "    )\n",
    "\n",
    "    # Inject nulls into some numeric columns using Polars\n",
    "    df = df.with_columns([\n",
    "        pl.when(pl.int_range(pl.len()).mod(7) == 0)\n",
    "          .then(None)\n",
    "          .otherwise(pl.col(\"num_0\"))\n",
    "          .alias(\"num_0\"),\n",
    "        pl.when(pl.int_range(pl.len()).mod(6) == 0)\n",
    "          .then(None)\n",
    "          .otherwise(pl.col(\"num_3\"))\n",
    "          .alias(\"num_3\"),\n",
    "        pl.when(pl.int_range(pl.len()).mod(8) == 0)\n",
    "          .then(None)\n",
    "          .otherwise(pl.col(\"num_7\"))\n",
    "          .alias(\"num_7\"),\n",
    "    ])\n",
    "\n",
    "    # High-null column (99% null) \u2014 Polars native\n",
    "    df = df.with_columns(\n",
    "        pl.when(pl.int_range(pl.len()).mod(100) == 0)\n",
    "          .then(pl.lit(0.5))\n",
    "          .otherwise(None)\n",
    "          .cast(pl.Float64)\n",
    "          .alias(\"num_mostly_null\"),\n",
    "    )\n",
    "\n",
    "    # Text column\n",
    "    words = [\"great\", \"bad\", \"ok\", \"excellent\", \"poor\", \"fine\", \"amazing\", \"terrible\"]\n",
    "    df = df.with_columns(\n",
    "        pl.Series(\"text_feedback\", [\n",
    "            \" \".join(random.choices(words, k=random.randint(3, 14)))\n",
    "            for _ in range(n)\n",
    "        ]),\n",
    "    )\n",
    "\n",
    "    # Time column \u2014 pure Python dates wrapped in Polars\n",
    "    base_date = date(2023, 1, 1)\n",
    "    df = df.with_columns(\n",
    "        pl.Series(\"event_time\", [\n",
    "            base_date + timedelta(days=random.randint(0, 364))\n",
    "            for _ in range(n)\n",
    "        ]).cast(pl.Date),\n",
    "    )\n",
    "\n",
    "    # Entity ID\n",
    "    df = df.with_columns(\n",
    "        pl.Series(\"entity_id\", [f\"ent_{i % 500}\" for i in range(n)]),\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Load or generate data\n",
    "if input_parquet_paths:\n",
    "    df_raw = load_parquet_polars(input_parquet_paths)\n",
    "    print(f\"Loaded {len(df_raw):,} rows from {len(input_parquet_paths)} file(s)\")\n",
    "else:\n",
    "    print(\"No input_parquet_paths provided \u2014 generating synthetic dataset\")\n",
    "    df_raw = generate_synthetic(task_type, n=5000, seed=sample_seed)\n",
    "    if time_col is None and \"event_time\" in df_raw.columns:\n",
    "        time_col = \"event_time\"\n",
    "    if not id_cols and \"entity_id\" in df_raw.columns:\n",
    "        id_cols = [\"entity_id\"]\n",
    "    if text_cols is None and \"text_feedback\" in df_raw.columns:\n",
    "        text_cols = [\"text_feedback\"]\n",
    "    print(f\"Synthetic dataset: {df_raw.shape}\")\n",
    "\n",
    "assert target_col in df_raw.columns, f\"target_col='{target_col}' not found in data\"\n",
    "print(f\"\\nTarget column: {target_col}\")\n",
    "print(f\"Shape: {df_raw.shape}\")\n",
    "df_raw.head(5)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# Drop specified columns and separate feature candidates\n",
    "# ---------------------------------------------------------------------------\n",
    "exclude_cols = set(drop_cols) | set(id_cols) | {target_col}\n",
    "if time_col:\n",
    "    exclude_cols.add(time_col)\n",
    "text_set = set(text_cols or [])\n",
    "exclude_cols |= text_set\n",
    "\n",
    "feature_candidates = [c for c in df_raw.columns if c not in exclude_cols]\n",
    "print(f\"Feature candidates: {len(feature_candidates)}\")\n",
    "print(f\"Excluded columns: {sorted(exclude_cols)}\")\n",
    "\n",
    "# Create EDA sample\n",
    "if len(df_raw) > max_rows_for_eda:\n",
    "    df_eda = df_raw.sample(n=max_rows_for_eda, seed=sample_seed)\n",
    "    print(f\"EDA sample: {len(df_eda):,} rows (sampled from {len(df_raw):,})\")\n",
    "else:\n",
    "    df_eda = df_raw\n",
    "    print(f\"EDA sample: full dataset ({len(df_eda):,} rows)\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## C \u2014 Type Inference & Column Categorization\n",
    "\n",
    "Infer which columns are numeric, categorical, or text based on dtype and\n",
    "cardinality. Integer columns with fewer than 20 unique values are treated\n",
    "as categorical."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# C \u2014 Type Inference (pure Polars)\n",
    "# ---------------------------------------------------------------------------\n",
    "MAX_INT_UNIQUE_FOR_CAT = 20\n",
    "\n",
    "def infer_column_types(\n",
    "    df: pl.DataFrame,\n",
    "    candidates: list[str],\n",
    "    explicit_num: list[str] | None,\n",
    "    explicit_cat: list[str] | None,\n",
    ") -> tuple[list[str], list[str]]:\n",
    "    if explicit_num is not None and explicit_cat is not None:\n",
    "        return (\n",
    "            [c for c in explicit_num if c in candidates],\n",
    "            [c for c in explicit_cat if c in candidates],\n",
    "        )\n",
    "    num, cat = [], []\n",
    "    for c in candidates:\n",
    "        dtype = df[c].dtype\n",
    "        if dtype in (pl.Utf8, pl.Categorical, pl.Boolean, pl.String):\n",
    "            cat.append(c)\n",
    "        elif dtype.is_numeric():\n",
    "            nunique = df[c].n_unique()\n",
    "            if dtype.is_integer() and nunique <= MAX_INT_UNIQUE_FOR_CAT:\n",
    "                cat.append(c)\n",
    "            else:\n",
    "                num.append(c)\n",
    "    if explicit_num is not None:\n",
    "        num = [c for c in explicit_num if c in candidates]\n",
    "    if explicit_cat is not None:\n",
    "        cat = [c for c in explicit_cat if c in candidates]\n",
    "    return num, cat\n",
    "\n",
    "\n",
    "inferred_num, inferred_cat = infer_column_types(\n",
    "    df_raw, feature_candidates, numeric_cols, categorical_cols\n",
    ")\n",
    "print(f\"Numeric features:     {len(inferred_num)}\")\n",
    "print(f\"Categorical features: {len(inferred_cat)}\")\n",
    "print(f\"Text features:        {len(text_cols or [])}\")\n",
    "\n",
    "# Build audit table entirely in Polars\n",
    "audit_rows = []\n",
    "for c in feature_candidates:\n",
    "    col = df_raw[c]\n",
    "    null_frac = col.null_count() / len(df_raw)\n",
    "    nunique = col.n_unique()\n",
    "    role = \"numeric\" if c in inferred_num else (\"categorical\" if c in inferred_cat else \"other\")\n",
    "    sample_vals = str(col.drop_nulls().head(3).to_list())[:80]\n",
    "    audit_rows.append({\n",
    "        \"column\": c, \"dtype\": str(col.dtype),\n",
    "        \"null_pct\": round(null_frac * 100, 2), \"n_unique\": nunique,\n",
    "        \"role\": role, \"sample\": sample_vals,\n",
    "    })\n",
    "\n",
    "audit_df = pl.DataFrame(audit_rows)\n",
    "report[\"column_audit\"] = audit_rows\n",
    "print(\"\\nColumn audit:\")\n",
    "audit_df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## D \u2014 Data Quality Audit\n",
    "\n",
    "A thorough, plot-heavy audit of data quality covering missingness,\n",
    "cardinality, distributions, outliers, and the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D.1 \u2014 Missingness\n",
    "\n",
    "Columns with more than the `missingness_drop_threshold` (default 98%) null\n",
    "fraction are dropped. We visualize the top columns by null rate and show a\n",
    "missingness matrix on a sample."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# D.1 \u2014 Missingness analysis (Polars)\n",
    "# ---------------------------------------------------------------------------\n",
    "# Compute null fractions in one Polars expression\n",
    "null_stats = (\n",
    "    df_raw.select(feature_candidates)\n",
    "    .null_count()\n",
    "    .unpivot(variable_name=\"feature\", value_name=\"null_count\")\n",
    "    .with_columns(\n",
    "        (pl.col(\"null_count\") / len(df_raw)).alias(\"null_frac\")\n",
    "    )\n",
    "    .sort(\"null_frac\", descending=True)\n",
    ")\n",
    "\n",
    "# Columns to drop for high missingness\n",
    "high_null_cols = null_stats.filter(\n",
    "    pl.col(\"null_frac\") > missingness_drop_threshold\n",
    ")[\"feature\"].to_list()\n",
    "\n",
    "for c in high_null_cols:\n",
    "    frac = null_stats.filter(pl.col(\"feature\") == c)[\"null_frac\"].item()\n",
    "    report[\"dropped_features\"].append({\n",
    "        \"feature\": c,\n",
    "        \"reason\": f\"null_frac={frac:.3f} > {missingness_drop_threshold}\",\n",
    "    })\n",
    "print(f\"Dropping {len(high_null_cols)} columns for high missingness: {high_null_cols}\")\n",
    "\n",
    "# --- Plot: top 50 columns by null fraction ---\n",
    "top_null = null_stats.head(50)\n",
    "feats = top_null[\"feature\"].to_list()\n",
    "fracs = top_null[\"null_frac\"].to_list()\n",
    "fig, ax = plt.subplots(figsize=(12, max(5, len(feats) * 0.25)))\n",
    "ax.barh(range(len(feats)), fracs, color=\"#e74c3c\", alpha=0.8)\n",
    "ax.set_yticks(range(len(feats)))\n",
    "ax.set_yticklabels(feats, fontsize=8)\n",
    "ax.set_xlabel(\"Null Fraction\")\n",
    "ax.set_title(\"Top 50 Columns by Null Fraction\")\n",
    "ax.axvline(missingness_drop_threshold, color=\"black\", ls=\"--\", lw=1,\n",
    "           label=f\"Drop threshold ({missingness_drop_threshold})\")\n",
    "ax.legend(fontsize=9)\n",
    "ax.invert_yaxis()\n",
    "fig.tight_layout()\n",
    "save_plot(fig, \"d1_null_fraction_bar.png\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Missingness matrix** \u2014 each row is an observation, each column is a feature.\n",
    "White pixels indicate null values. Correlated missingness patterns suggest\n",
    "structural reasons (e.g., entire form sections skipped)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# --- Plot: missingness matrix (Polars -> single to_numpy at the end) ---\n",
    "miss_cols = null_stats.filter(pl.col(\"null_frac\") > 0)[\"feature\"].to_list()[:40]\n",
    "if miss_cols:\n",
    "    sample_for_matrix = safe_sample(df_eda, 500, sample_seed)\n",
    "    # Build boolean null matrix entirely in Polars, then convert once\n",
    "    miss_df = sample_for_matrix.select([\n",
    "        pl.col(c).is_null().cast(pl.Float32).alias(c)\n",
    "        for c in miss_cols\n",
    "    ])\n",
    "    # Single .to_numpy() call at the matplotlib boundary\n",
    "    miss_matrix = miss_df.to_numpy()\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(max(6, len(miss_cols) * 0.3), 6))\n",
    "    ax.imshow(miss_matrix, aspect=\"auto\", cmap=\"gray_r\", interpolation=\"nearest\")\n",
    "    ax.set_xticks(range(len(miss_cols)))\n",
    "    ax.set_xticklabels(miss_cols, rotation=90, fontsize=7)\n",
    "    ax.set_ylabel(\"Row index (sample)\")\n",
    "    ax.set_title(\"Missingness Matrix (white = null)\")\n",
    "    fig.tight_layout()\n",
    "    save_plot(fig, \"d1_missingness_matrix.png\")\n",
    "else:\n",
    "    print(\"No columns with nulls \u2014 skipping missingness matrix.\")\n",
    "\n",
    "# Remove high-null columns from feature lists\n",
    "inferred_num = [c for c in inferred_num if c not in high_null_cols]\n",
    "inferred_cat = [c for c in inferred_cat if c not in high_null_cols]\n",
    "feature_candidates = [c for c in feature_candidates if c not in high_null_cols]\n",
    "report[\"missingness_summary\"] = {\n",
    "    \"total_features\": len(feature_candidates) + len(high_null_cols),\n",
    "    \"dropped_high_null\": high_null_cols,\n",
    "    \"remaining\": len(feature_candidates),\n",
    "}\n",
    "print(f\"Remaining features after missingness drop: {len(feature_candidates)}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D.2 \u2014 Cardinality & Category Health\n",
    "\n",
    "High-cardinality categoricals (> `high_cardinality_threshold` unique values)\n",
    "may cause memory issues with one-hot encoding and are flagged. We also show\n",
    "frequency distributions for top categoricals."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# D.2 \u2014 Cardinality analysis (Polars)\n",
    "# ---------------------------------------------------------------------------\n",
    "if inferred_cat:\n",
    "    card_data = pl.DataFrame({\n",
    "        \"feature\": inferred_cat,\n",
    "        \"n_unique\": [df_raw[c].n_unique() for c in inferred_cat],\n",
    "    }).sort(\"n_unique\", descending=True)\n",
    "\n",
    "    high_card = card_data.filter(\n",
    "        pl.col(\"n_unique\") > high_cardinality_threshold\n",
    "    )[\"feature\"].to_list()\n",
    "    if high_card:\n",
    "        print(f\"High-cardinality categoricals (>{high_cardinality_threshold} unique): {high_card}\")\n",
    "        for c in high_card:\n",
    "            report[\"high_risk_features\"].append({\"feature\": c, \"reason\": \"high_cardinality\"})\n",
    "    else:\n",
    "        print(\"No high-cardinality categoricals flagged.\")\n",
    "\n",
    "    # --- Plot: n_unique (log scale) ---\n",
    "    feats_c = card_data[\"feature\"].to_list()\n",
    "    nuniques_c = card_data[\"n_unique\"].to_list()\n",
    "    fig, ax = plt.subplots(figsize=(10, max(4, len(feats_c) * 0.3)))\n",
    "    ax.barh(range(len(feats_c)), nuniques_c, color=\"#3498db\", alpha=0.8)\n",
    "    ax.set_yticks(range(len(feats_c)))\n",
    "    ax.set_yticklabels(feats_c, fontsize=8)\n",
    "    ax.set_xscale(\"log\")\n",
    "    ax.set_xlabel(\"Number of Unique Values (log scale)\")\n",
    "    ax.set_title(\"Categorical Feature Cardinality\")\n",
    "    ax.axvline(high_cardinality_threshold, color=\"red\", ls=\"--\", lw=1,\n",
    "               label=f\"Threshold ({high_cardinality_threshold})\")\n",
    "    ax.legend(fontsize=9)\n",
    "    ax.invert_yaxis()\n",
    "    fig.tight_layout()\n",
    "    save_plot(fig, \"d2_cardinality.png\")\n",
    "\n",
    "    # --- Plot: frequency bar charts for top categoricals ---\n",
    "    top_cats_to_show = inferred_cat[:6]\n",
    "    if top_cats_to_show:\n",
    "        n_show = len(top_cats_to_show)\n",
    "        ncols = min(3, n_show)\n",
    "        nrows = (n_show + ncols - 1) // ncols\n",
    "        fig, axes = plt.subplots(nrows, ncols, figsize=(5 * ncols, 4 * nrows))\n",
    "        axes_flat = [axes] if n_show == 1 else list(np.array(axes).flatten())\n",
    "        for i, c in enumerate(top_cats_to_show):\n",
    "            vc = (\n",
    "                df_eda.select(pl.col(c).cast(pl.Utf8).fill_null(\"__NULL__\"))\n",
    "                .to_series()\n",
    "                .value_counts()\n",
    "                .sort(\"count\", descending=True)\n",
    "            )\n",
    "            top_vals = vc.head(15)\n",
    "            other_count = vc[\"count\"].sum() - top_vals[\"count\"].sum()\n",
    "            labels = top_vals[c].to_list()\n",
    "            counts = top_vals[\"count\"].to_list()\n",
    "            if other_count > 0:\n",
    "                labels.append(\"__OTHER__\")\n",
    "                counts.append(other_count)\n",
    "            axes_flat[i].barh(range(len(labels)), counts, color=\"#2ecc71\", alpha=0.8)\n",
    "            axes_flat[i].set_yticks(range(len(labels)))\n",
    "            axes_flat[i].set_yticklabels(labels, fontsize=7)\n",
    "            axes_flat[i].set_title(c, fontsize=10)\n",
    "            axes_flat[i].invert_yaxis()\n",
    "        for j in range(i + 1, len(axes_flat)):\n",
    "            axes_flat[j].set_visible(False)\n",
    "        fig.suptitle(\"Category Frequency Distributions (top 15 + OTHER)\", fontsize=12, y=1.01)\n",
    "        fig.tight_layout()\n",
    "        save_plot(fig, \"d2_category_frequencies.png\")\n",
    "else:\n",
    "    print(\"No categorical features to analyze.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D.3 \u2014 Numeric Distributions & Outliers\n",
    "\n",
    "Histograms and boxplots for numeric features. Skewness and kurtosis are\n",
    "computed in Polars. If `enable_outlier_clipping` is True, we show\n",
    "before/after histograms for winsorized features."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# D.3 \u2014 Numeric distributions (Polars + matplotlib)\n",
    "# ---------------------------------------------------------------------------\n",
    "if inferred_num:\n",
    "    # --- Histograms ---\n",
    "    num_to_plot = inferred_num[:12]\n",
    "    ncols = min(4, len(num_to_plot))\n",
    "    nrows = (len(num_to_plot) + ncols - 1) // ncols\n",
    "    fig, axes = plt.subplots(nrows, ncols, figsize=(4 * ncols, 3.5 * nrows))\n",
    "    axes_flat = [axes] if len(num_to_plot) == 1 else list(np.array(axes).flatten())\n",
    "    for i, c in enumerate(num_to_plot):\n",
    "        vals = df_eda[c].drop_nulls().to_list()\n",
    "        if not vals:\n",
    "            axes_flat[i].set_title(f\"{c} (all null)\")\n",
    "            continue\n",
    "        axes_flat[i].hist(vals, bins=50, color=\"#9b59b6\", alpha=0.7, edgecolor=\"white\", linewidth=0.3)\n",
    "        axes_flat[i].set_title(c, fontsize=9)\n",
    "        axes_flat[i].tick_params(labelsize=7)\n",
    "    for j in range(i + 1, len(axes_flat)):\n",
    "        axes_flat[j].set_visible(False)\n",
    "    fig.suptitle(\"Numeric Feature Distributions\", fontsize=12, y=1.01)\n",
    "    fig.tight_layout()\n",
    "    save_plot(fig, \"d3_numeric_histograms.png\")\n",
    "\n",
    "    # --- Boxplots ---\n",
    "    box_cols = inferred_num[:10]\n",
    "    box_data = [df_eda[c].drop_nulls().to_list() for c in box_cols]\n",
    "    fig, ax = plt.subplots(figsize=(max(6, len(box_cols) * 0.8), 5))\n",
    "    ax.boxplot(box_data, vert=True, patch_artist=True,\n",
    "               boxprops=dict(facecolor=\"#3498db\", alpha=0.6))\n",
    "    ax.set_xticks(range(1, len(box_cols) + 1))\n",
    "    ax.set_xticklabels(box_cols, rotation=45, ha=\"right\", fontsize=8)\n",
    "    ax.set_title(\"Numeric Feature Boxplots\")\n",
    "    fig.tight_layout()\n",
    "    save_plot(fig, \"d3_numeric_boxplots.png\")\n",
    "else:\n",
    "    print(\"No numeric features.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# Skewness & kurtosis \u2014 computed in Polars\n",
    "# ---------------------------------------------------------------------------\n",
    "if inferred_num:\n",
    "    skew_rows = []\n",
    "    for c in inferred_num:\n",
    "        col = df_eda[c].drop_nulls()\n",
    "        n_vals = col.len()\n",
    "        if n_vals < 4:\n",
    "            continue\n",
    "        sk = col.skew()\n",
    "        ku = col.kurtosis()\n",
    "        skew_rows.append({\n",
    "            \"feature\": c,\n",
    "            \"skewness\": round(sk, 3) if sk is not None else 0.0,\n",
    "            \"kurtosis\": round(ku, 3) if ku is not None else 0.0,\n",
    "        })\n",
    "\n",
    "    skew_df = pl.DataFrame(skew_rows).sort(\"skewness\", descending=True)\n",
    "    print(\"Skewness > 1 or < -1 suggests heavy tails; consider log transform.\")\n",
    "    print(\"Kurtosis > 3 (excess) indicates leptokurtic / outlier-prone distribution.\\n\")\n",
    "    skew_df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# Outlier clipping before/after \u2014 Polars quantile + clip\n",
    "# ---------------------------------------------------------------------------\n",
    "if enable_outlier_clipping and inferred_num:\n",
    "    clip_demo_cols = inferred_num[:3]\n",
    "    fig, axes = plt.subplots(len(clip_demo_cols), 2, figsize=(10, 3.5 * len(clip_demo_cols)))\n",
    "    if len(clip_demo_cols) == 1:\n",
    "        axes = [axes]  # keep as list of pairs\n",
    "        axes = [axes]\n",
    "\n",
    "    for i, c in enumerate(clip_demo_cols):\n",
    "        col = df_eda[c].drop_nulls()\n",
    "        if col.len() == 0:\n",
    "            continue\n",
    "        lo = col.quantile(winsorize_limits[0])\n",
    "        hi = col.quantile(winsorize_limits[1])\n",
    "        before = col.to_list()\n",
    "        after = col.clip(lo, hi).to_list()\n",
    "\n",
    "        ax_row = axes[i] if len(clip_demo_cols) > 1 else axes[0]\n",
    "        ax_row[0].hist(before, bins=50, color=\"#e74c3c\", alpha=0.7, edgecolor=\"white\", linewidth=0.3)\n",
    "        ax_row[0].set_title(f\"{c} \u2014 before clipping\", fontsize=9)\n",
    "        ax_row[1].hist(after, bins=50, color=\"#2ecc71\", alpha=0.7, edgecolor=\"white\", linewidth=0.3)\n",
    "        ax_row[1].set_title(f\"{c} \u2014 after clipping [{lo:.2f}, {hi:.2f}]\", fontsize=9)\n",
    "\n",
    "    fig.suptitle(\"Outlier Clipping (Winsorization) Before / After\", fontsize=12, y=1.01)\n",
    "    fig.tight_layout()\n",
    "    save_plot(fig, \"d3_outlier_clipping.png\")\n",
    "    print(f\"Winsorize limits: {winsorize_limits}\")\n",
    "elif not enable_outlier_clipping:\n",
    "    print(\"Outlier clipping disabled.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D.4 \u2014 Target Exploration\n",
    "\n",
    "Distribution of the target variable. For classification: class balance bar\n",
    "chart. For regression: histogram. If a `time_col` is available, we plot\n",
    "the target rate (or mean) over time. If `group_cols` are provided, we show\n",
    "target rates by group (purely descriptive \u2014 no leakage into features)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# D.4 \u2014 Target exploration (Polars)\n",
    "# ---------------------------------------------------------------------------\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "if task_type in (\"binary\", \"multiclass\"):\n",
    "    vc = df_raw[target_col].value_counts().sort(target_col)\n",
    "    labels = vc[target_col].cast(pl.Utf8).to_list()\n",
    "    counts = vc[\"count\"].to_list()\n",
    "    total = sum(counts)\n",
    "    colors = plt.cm.Set2([i / max(len(labels), 1) for i in range(len(labels))])\n",
    "    ax.bar(labels, counts, color=colors, edgecolor=\"white\")\n",
    "    ax.set_xlabel(\"Class\")\n",
    "    ax.set_ylabel(\"Count\")\n",
    "    ax.set_title(f\"Target Distribution ({target_col})\")\n",
    "    for j, (lbl, cnt) in enumerate(zip(labels, counts)):\n",
    "        ax.text(j, cnt, f\"{cnt:,}\\n({cnt/total:.1%})\", ha=\"center\", va=\"bottom\", fontsize=9)\n",
    "else:\n",
    "    vals = df_raw[target_col].drop_nulls().to_list()\n",
    "    ax.hist(vals, bins=60, color=\"#e67e22\", alpha=0.8, edgecolor=\"white\", linewidth=0.3)\n",
    "    ax.set_xlabel(target_col)\n",
    "    ax.set_ylabel(\"Count\")\n",
    "    ax.set_title(f\"Target Distribution ({target_col}) \u2014 Regression\")\n",
    "fig.tight_layout()\n",
    "save_plot(fig, \"d4_target_distribution.png\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# --- Target over time ---\n",
    "if time_col and time_col in df_raw.columns:\n",
    "    time_agg = (\n",
    "        df_raw.select([time_col, target_col])\n",
    "        .drop_nulls()\n",
    "        .with_columns(pl.col(time_col).cast(pl.Date).dt.truncate(\"1mo\").alias(\"month\"))\n",
    "        .group_by(\"month\")\n",
    "        .agg([\n",
    "            pl.col(target_col).mean().alias(\"target_rate\"),\n",
    "            pl.col(target_col).count().alias(\"n\"),\n",
    "        ])\n",
    "        .sort(\"month\")\n",
    "    )\n",
    "    months = time_agg[\"month\"].to_list()\n",
    "    rates = time_agg[\"target_rate\"].to_list()\n",
    "    ylabel = \"Target Rate (mean)\" if task_type in (\"binary\", \"multiclass\") else \"Target Mean\"\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 4))\n",
    "    ax.plot(months, rates, marker=\"o\", color=\"#2980b9\", linewidth=2, markersize=4)\n",
    "    ax.set_xlabel(\"Month\")\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.set_title(\"Target Over Time (monthly)\")\n",
    "    ax.tick_params(axis=\"x\", rotation=45)\n",
    "    fig.tight_layout()\n",
    "    save_plot(fig, \"d4_target_over_time.png\")\n",
    "else:\n",
    "    print(\"No time_col \u2014 skipping target-over-time plot.\")\n",
    "\n",
    "# --- Target by group ---\n",
    "if group_cols:\n",
    "    for gc in group_cols[:2]:\n",
    "        if gc not in df_raw.columns:\n",
    "            continue\n",
    "        grp = (\n",
    "            df_raw.group_by(gc)\n",
    "            .agg([\n",
    "                pl.col(target_col).mean().alias(\"target_rate\"),\n",
    "                pl.col(target_col).count().alias(\"n\"),\n",
    "            ])\n",
    "            .sort(\"target_rate\", descending=True)\n",
    "            .head(20)\n",
    "        )\n",
    "        fig, ax = plt.subplots(figsize=(10, max(4, len(grp) * 0.3)))\n",
    "        ax.barh(range(len(grp)), grp[\"target_rate\"].to_list(), color=\"#1abc9c\", alpha=0.8)\n",
    "        ax.set_yticks(range(len(grp)))\n",
    "        ax.set_yticklabels(grp[gc].cast(pl.Utf8).to_list(), fontsize=8)\n",
    "        ax.set_xlabel(\"Target Rate / Mean\")\n",
    "        ax.set_title(f\"Target by {gc} (top 20)\")\n",
    "        ax.invert_yaxis()\n",
    "        fig.tight_layout()\n",
    "        save_plot(fig, f\"d4_target_by_{gc}.png\")\n",
    "else:\n",
    "    print(\"No group_cols \u2014 skipping target-by-group plot.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## E \u2014 Baseline Feature Scoring\n",
    "\n",
    "Multiple independent univariate \"feature quality\" scores. We compute\n",
    "rankings per method and then build a **consensus score** that combines\n",
    "all available rankings.\n",
    "\n",
    "> **Note:** sklearn functions require numpy arrays. We prepare data in\n",
    "> Polars (fill nulls, cast dtypes) then convert once at the sklearn\n",
    "> boundary.\n",
    "\n",
    "| Method | Applies to | What it measures |\n",
    "|---|---|---|\n",
    "| Pearson/Spearman correlation | Numeric | Linear/monotonic association |\n",
    "| ANOVA F-value | Numeric (classification) | Between-class variance |\n",
    "| Mutual information | Both | Any dependency (nonlinear included) |\n",
    "| Univariate ROC-AUC | Numeric (binary) | Discriminative power as a score |\n",
    "| Chi-squared | Categorical | Association with target |\n",
    "| Near-zero variance | Numeric | Whether feature is constant |"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# E \u2014 Baseline feature scoring: numeric features\n",
    "# ---------------------------------------------------------------------------\n",
    "score_rankings: dict[str, dict[str, float]] = {}\n",
    "\n",
    "# Prepare data in Polars: fill nulls with median, drop target nulls\n",
    "df_score_sample = safe_sample(df_raw, 50_000, sample_seed).drop_nulls(subset=[target_col])\n",
    "\n",
    "if inferred_num:\n",
    "    # Fill nulls with median in Polars\n",
    "    df_num_filled = df_score_sample.select(\n",
    "        [pl.col(c).fill_null(pl.col(c).median()) for c in inferred_num]\n",
    "        + [pl.col(target_col)]\n",
    "    )\n",
    "\n",
    "    # Single conversion to numpy at the sklearn boundary\n",
    "    X_num = df_num_filled.select(inferred_num).to_numpy().astype(np.float64)\n",
    "    y_score = df_num_filled[target_col].to_numpy()\n",
    "    X_num = np.nan_to_num(X_num, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "    # 1) Correlation with target (via scipy \u2014 no Polars equivalent for point-biserial)\n",
    "    corr_scores = {}\n",
    "    for i, c in enumerate(inferred_num):\n",
    "        if task_type == \"regression\":\n",
    "            r, _ = sp_stats.spearmanr(X_num[:, i], y_score, nan_policy=\"omit\")\n",
    "        else:\n",
    "            r, _ = sp_stats.pointbiserialr(y_score, X_num[:, i])\n",
    "        corr_scores[c] = abs(float(r)) if not (r is None or math.isnan(r)) else 0.0\n",
    "    score_rankings[\"correlation\"] = corr_scores\n",
    "    print(f\"Correlation scores computed for {len(corr_scores)} numeric features.\")\n",
    "\n",
    "    # 2) ANOVA F-value\n",
    "    if task_type in (\"binary\", \"multiclass\"):\n",
    "        f_vals, _ = f_classif(X_num, y_score)\n",
    "    else:\n",
    "        f_vals, _ = f_regression(X_num, y_score)\n",
    "    anova_scores = {}\n",
    "    for i, c in enumerate(inferred_num):\n",
    "        v = float(f_vals[i])\n",
    "        anova_scores[c] = v if not math.isnan(v) else 0.0\n",
    "    score_rankings[\"anova_f\"] = anova_scores\n",
    "    print(\"ANOVA F-scores computed.\")\n",
    "\n",
    "    # 3) Mutual information\n",
    "    if enable_mutual_info:\n",
    "        if task_type in (\"binary\", \"multiclass\"):\n",
    "            mi = mutual_info_classif(X_num, y_score, random_state=random_state, n_neighbors=5)\n",
    "        else:\n",
    "            mi = mutual_info_regression(X_num, y_score, random_state=random_state, n_neighbors=5)\n",
    "        score_rankings[\"mutual_info\"] = {c: float(mi[i]) for i, c in enumerate(inferred_num)}\n",
    "        print(\"Mutual information scores computed.\")\n",
    "\n",
    "    # 4) Univariate ROC-AUC (binary only)\n",
    "    if task_type == \"binary\":\n",
    "        auc_scores = {}\n",
    "        for i, c in enumerate(inferred_num):\n",
    "            try:\n",
    "                auc = roc_auc_score(y_score, X_num[:, i])\n",
    "                auc_scores[c] = max(auc, 1 - auc)\n",
    "            except Exception:\n",
    "                auc_scores[c] = 0.5\n",
    "        score_rankings[\"univariate_auc\"] = auc_scores\n",
    "        print(\"Univariate AUC scores computed.\")\n",
    "\n",
    "    # 5) Variance (Polars-native)\n",
    "    var_scores = {}\n",
    "    for c in inferred_num:\n",
    "        v = df_score_sample[c].var()\n",
    "        var_scores[c] = float(v) if v is not None else 0.0\n",
    "    near_zero_var = [c for c, v in var_scores.items() if v < 1e-10]\n",
    "    if near_zero_var:\n",
    "        print(f\"Near-zero variance features: {near_zero_var}\")\n",
    "        for c in near_zero_var:\n",
    "            report[\"dropped_features\"].append({\"feature\": c, \"reason\": \"near_zero_variance\"})\n",
    "    score_rankings[\"variance\"] = var_scores\n",
    "\n",
    "print(f\"\\nScoring methods computed: {list(score_rankings.keys())}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# E \u2014 Baseline feature scoring: categorical features\n",
    "# ---------------------------------------------------------------------------\n",
    "if inferred_cat:\n",
    "    df_cat_sample = df_score_sample.select(inferred_cat + [target_col])\n",
    "    y_cat = df_cat_sample[target_col].to_numpy()\n",
    "\n",
    "    # Ordinal-encode in Polars (no pandas needed)\n",
    "    df_cat_encoded = df_cat_sample.select(inferred_cat)\n",
    "    for c in inferred_cat:\n",
    "        uniques = df_cat_encoded[c].cast(pl.Utf8).fill_null(\"__MISSING__\").unique().sort()\n",
    "        mapping = {v: float(i) for i, v in enumerate(uniques.to_list())}\n",
    "        df_cat_encoded = df_cat_encoded.with_columns(\n",
    "            pl.col(c).cast(pl.Utf8).fill_null(\"__MISSING__\")\n",
    "            .replace_strict(mapping, default=-1.0)\n",
    "            .cast(pl.Float64)\n",
    "            .alias(c)\n",
    "        )\n",
    "    X_cat_enc = df_cat_encoded.to_numpy()\n",
    "\n",
    "    # Chi-squared (classification only)\n",
    "    if task_type in (\"binary\", \"multiclass\"):\n",
    "        X_cat_nonneg = X_cat_enc - X_cat_enc.min(axis=0)\n",
    "        chi2_vals, _ = chi2(X_cat_nonneg, y_cat)\n",
    "        chi2_scores = {}\n",
    "        for i, c in enumerate(inferred_cat):\n",
    "            v = float(chi2_vals[i])\n",
    "            chi2_scores[c] = v if not math.isnan(v) else 0.0\n",
    "        score_rankings[\"chi2\"] = chi2_scores\n",
    "        print(f\"Chi-squared scores computed for {len(chi2_scores)} categorical features.\")\n",
    "\n",
    "    # Mutual information for categoricals\n",
    "    if enable_mutual_info:\n",
    "        if task_type in (\"binary\", \"multiclass\"):\n",
    "            mi_cat = mutual_info_classif(X_cat_enc, y_cat, discrete_features=True, random_state=random_state)\n",
    "        else:\n",
    "            mi_cat = mutual_info_regression(X_cat_enc, y_cat, discrete_features=True, random_state=random_state)\n",
    "        score_rankings[\"mutual_info_cat\"] = {c: float(mi_cat[i]) for i, c in enumerate(inferred_cat)}\n",
    "        print(\"Mutual info (categorical) computed.\")\n",
    "\n",
    "    # Target rate per category plots (classification)\n",
    "    if task_type in (\"binary\", \"multiclass\"):\n",
    "        cats_to_plot = inferred_cat[:4]\n",
    "        if cats_to_plot:\n",
    "            ncols_p = min(2, len(cats_to_plot))\n",
    "            nrows_p = (len(cats_to_plot) + ncols_p - 1) // ncols_p\n",
    "            fig, axes = plt.subplots(nrows_p, ncols_p, figsize=(6 * ncols_p, 4 * nrows_p))\n",
    "            axes_flat = [axes] if len(cats_to_plot) == 1 else list(np.array(axes).flatten())\n",
    "            for idx, c in enumerate(cats_to_plot):\n",
    "                grp = (\n",
    "                    df_score_sample.select([c, target_col])\n",
    "                    .with_columns(pl.col(c).cast(pl.Utf8).fill_null(\"__NULL__\"))\n",
    "                    .group_by(c)\n",
    "                    .agg([\n",
    "                        pl.col(target_col).mean().alias(\"target_rate\"),\n",
    "                        pl.col(target_col).count().alias(\"n\"),\n",
    "                    ])\n",
    "                    .filter(pl.col(\"n\") >= rare_category_min_count)\n",
    "                    .sort(\"target_rate\", descending=True)\n",
    "                    .head(15)\n",
    "                )\n",
    "                axes_flat[idx].barh(\n",
    "                    range(len(grp)), grp[\"target_rate\"].to_list(),\n",
    "                    color=\"#e67e22\", alpha=0.8,\n",
    "                )\n",
    "                axes_flat[idx].set_yticks(range(len(grp)))\n",
    "                axes_flat[idx].set_yticklabels(grp[c].to_list(), fontsize=7)\n",
    "                axes_flat[idx].set_xlabel(\"Target Rate\")\n",
    "                axes_flat[idx].set_title(f\"Target Rate by {c}\", fontsize=10)\n",
    "                axes_flat[idx].invert_yaxis()\n",
    "            for j in range(idx + 1, len(axes_flat)):\n",
    "                axes_flat[j].set_visible(False)\n",
    "            fig.suptitle(\"Target Rate per Category (min count filter applied)\", fontsize=12, y=1.01)\n",
    "            fig.tight_layout()\n",
    "            save_plot(fig, \"e_target_rate_by_category.png\")\n",
    "else:\n",
    "    print(\"No categorical features to score.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E \u2014 Univariate Score Plots & Consensus Ranking\n",
    "\n",
    "For each scoring method, we plot the top 30 features. Then we compute a\n",
    "**consensus score** by normalizing ranks across methods and averaging.\n",
    "Features that consistently rank high across methods are the most robust."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# E \u2014 Plot top features per scoring method\n",
    "# ---------------------------------------------------------------------------\n",
    "all_scored_methods = [m for m in score_rankings if m != \"variance\"]\n",
    "\n",
    "if all_scored_methods:\n",
    "    n_methods = len(all_scored_methods)\n",
    "    fig, axes = plt.subplots(n_methods, 1, figsize=(10, 4 * n_methods))\n",
    "    if n_methods == 1:\n",
    "        axes = [axes]\n",
    "    colors = [\"#e74c3c\", \"#3498db\", \"#2ecc71\", \"#9b59b6\", \"#e67e22\", \"#1abc9c\"]\n",
    "\n",
    "    for i, method in enumerate(all_scored_methods):\n",
    "        scores = score_rankings[method]\n",
    "        sorted_feats = sorted(scores.items(), key=lambda x: x[1], reverse=True)[:30]\n",
    "        names = [f[0] for f in sorted_feats]\n",
    "        vals = [f[1] for f in sorted_feats]\n",
    "        axes[i].barh(range(len(names)), vals, color=colors[i % len(colors)], alpha=0.8)\n",
    "        axes[i].set_yticks(range(len(names)))\n",
    "        axes[i].set_yticklabels(names, fontsize=7)\n",
    "        axes[i].set_title(f\"Top 30 by {method}\", fontsize=10)\n",
    "        axes[i].invert_yaxis()\n",
    "    fig.suptitle(\"Univariate Feature Scores by Method\", fontsize=13, y=1.0)\n",
    "    fig.tight_layout()\n",
    "    save_plot(fig, \"e_univariate_scores.png\")\n",
    "\n",
    "for method, scores in score_rankings.items():\n",
    "    ranked = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    report[\"univariate_scores\"][method] = [{\"feature\": f, \"score\": round(s, 6)} for f, s in ranked]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# E \u2014 Consensus ranking (pure Python, no numpy)\n",
    "# ---------------------------------------------------------------------------\n",
    "all_features_scored = sorted({f for scores in score_rankings.values() for f in scores})\n",
    "methods_for_consensus = [m for m in score_rankings if m != \"variance\"]\n",
    "\n",
    "if methods_for_consensus and all_features_scored:\n",
    "    n_f = len(all_features_scored)\n",
    "    rank_matrix: dict[str, list[float]] = {f: [] for f in all_features_scored}\n",
    "\n",
    "    for method in methods_for_consensus:\n",
    "        scores = score_rankings[method]\n",
    "        sorted_feats = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "        feat_to_rank = {f: rank + 1 for rank, (f, _) in enumerate(sorted_feats)}\n",
    "        for f in all_features_scored:\n",
    "            r = feat_to_rank.get(f, n_f)\n",
    "            rank_matrix[f].append(r / n_f)\n",
    "\n",
    "    # Consensus = 1 - mean(normalized_rank)\n",
    "    consensus = {f: 1.0 - sum(ranks) / len(ranks) for f, ranks in rank_matrix.items()}\n",
    "    consensus_sorted = sorted(consensus.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    report[\"consensus_ranking\"] = [\n",
    "        {\"feature\": f, \"consensus_score\": round(s, 4)} for f, s in consensus_sorted\n",
    "    ]\n",
    "\n",
    "    # --- Plot: consensus top 30 ---\n",
    "    top_consensus = consensus_sorted[:30]\n",
    "    fig, ax = plt.subplots(figsize=(10, max(5, len(top_consensus) * 0.25)))\n",
    "    ax.barh(range(len(top_consensus)), [s for _, s in top_consensus], color=\"#2c3e50\", alpha=0.85)\n",
    "    ax.set_yticks(range(len(top_consensus)))\n",
    "    ax.set_yticklabels([f for f, _ in top_consensus], fontsize=8)\n",
    "    ax.set_xlabel(\"Consensus Score (higher = more consistently important)\")\n",
    "    ax.set_title(\"Top 30 Features \u2014 Consensus Ranking Across Methods\")\n",
    "    ax.invert_yaxis()\n",
    "    fig.tight_layout()\n",
    "    save_plot(fig, \"e_consensus_ranking.png\")\n",
    "\n",
    "    # --- Rank agreement: how many methods put feature in top-k ---\n",
    "    top_k = 15\n",
    "    method_top_sets = {}\n",
    "    for method in methods_for_consensus:\n",
    "        scores = score_rankings[method]\n",
    "        top_feats = sorted(scores.items(), key=lambda x: x[1], reverse=True)[:top_k]\n",
    "        method_top_sets[method] = {f for f, _ in top_feats}\n",
    "\n",
    "    agreement_count: dict[str, int] = {}\n",
    "    for fset in method_top_sets.values():\n",
    "        for f in fset:\n",
    "            agreement_count[f] = agreement_count.get(f, 0) + 1\n",
    "    agreement_sorted = sorted(agreement_count.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, max(4, len(agreement_sorted) * 0.22)))\n",
    "    ax.barh(range(len(agreement_sorted)), [c for _, c in agreement_sorted], color=\"#16a085\", alpha=0.8)\n",
    "    ax.set_yticks(range(len(agreement_sorted)))\n",
    "    ax.set_yticklabels([f for f, _ in agreement_sorted], fontsize=7)\n",
    "    ax.set_xlabel(f\"# Methods placing feature in top-{top_k}\")\n",
    "    ax.set_title(f\"Rank Agreement: Features in Top-{top_k} Across {len(methods_for_consensus)} Methods\")\n",
    "    ax.invert_yaxis()\n",
    "    fig.tight_layout()\n",
    "    save_plot(fig, \"e_rank_agreement.png\")\n",
    "    print(f\"Consensus ranking computed across {len(methods_for_consensus)} methods.\")\n",
    "else:\n",
    "    consensus_sorted = []\n",
    "    print(\"Not enough scoring data for consensus ranking.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## F \u2014 Feature Engineering\n",
    "\n",
    "All transformations are done in Polars. Features are added to the working\n",
    "dataframe and tracked in `engineered_features_created`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F.1 \u2014 Missingness Indicators\n",
    "\n",
    "For columns with meaningful missingness (> 1%), we create binary `is_null`\n",
    "flags. These can carry signal if missingness is informative."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# F.1 \u2014 Missingness indicators (Polars)\n",
    "# ---------------------------------------------------------------------------\n",
    "df_eng = df_raw.clone()\n",
    "miss_indicator_cols = []\n",
    "\n",
    "null_frac_lookup = dict(zip(\n",
    "    null_stats[\"feature\"].to_list(),\n",
    "    null_stats[\"null_frac\"].to_list(),\n",
    "))\n",
    "\n",
    "new_cols = []\n",
    "for c in inferred_num + inferred_cat:\n",
    "    nf = null_frac_lookup.get(c, 0.0)\n",
    "    if 0.01 < nf < missingness_drop_threshold:\n",
    "        new_col = f\"{c}_is_null\"\n",
    "        new_cols.append(pl.col(c).is_null().cast(pl.Int8).alias(new_col))\n",
    "        miss_indicator_cols.append(new_col)\n",
    "\n",
    "if new_cols:\n",
    "    df_eng = df_eng.with_columns(new_cols)\n",
    "report[\"engineered_features_created\"].extend(miss_indicator_cols)\n",
    "print(f\"Created {len(miss_indicator_cols)} missingness indicator features.\")\n",
    "\n",
    "# --- Plot: missingness indicator vs target ---\n",
    "if miss_indicator_cols and task_type in (\"binary\", \"multiclass\"):\n",
    "    n_to_plot = min(6, len(miss_indicator_cols))\n",
    "    fig, axes = plt.subplots(1, n_to_plot, figsize=(4 * n_to_plot, 4))\n",
    "    if n_to_plot == 1:\n",
    "        axes = [axes]\n",
    "    for i, mc in enumerate(miss_indicator_cols[:n_to_plot]):\n",
    "        grp = (\n",
    "            df_eng.group_by(mc)\n",
    "            .agg(pl.col(target_col).mean().alias(\"target_rate\"))\n",
    "            .sort(mc)\n",
    "        )\n",
    "        labels = grp[mc].cast(pl.Utf8).to_list()\n",
    "        rates = grp[\"target_rate\"].to_list()\n",
    "        axes[i].bar(labels, rates, color=[\"#3498db\", \"#e74c3c\"][:len(labels)], alpha=0.8)\n",
    "        axes[i].set_title(mc.replace(\"_is_null\", \"\"), fontsize=9)\n",
    "        axes[i].set_xlabel(\"Is Null\")\n",
    "        axes[i].set_ylabel(\"Target Rate\")\n",
    "    fig.suptitle(\"Missingness Indicators vs Target Rate\", fontsize=12, y=1.02)\n",
    "    fig.tight_layout()\n",
    "    save_plot(fig, \"f1_missingness_indicators.png\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F.2 \u2014 Numeric Transforms\n",
    "\n",
    "For heavily-skewed positive features (|skewness| > 1), we apply `log1p`.\n",
    "We also show quantile-binned target rates for selected features."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# F.2 \u2014 Numeric transforms (Polars)\n",
    "# ---------------------------------------------------------------------------\n",
    "log_cols_created = []\n",
    "\n",
    "if inferred_num:\n",
    "    log_exprs = []\n",
    "    for c in inferred_num:\n",
    "        col = df_eng[c].drop_nulls()\n",
    "        if col.len() < 10:\n",
    "            continue\n",
    "        sk = col.skew()\n",
    "        mn = col.min()\n",
    "        if sk is not None and abs(sk) > 1.0 and mn is not None and mn >= 0:\n",
    "            new_col = f\"{c}_log1p\"\n",
    "            log_exprs.append(pl.col(c).fill_null(0).log1p().alias(new_col))\n",
    "            log_cols_created.append(new_col)\n",
    "\n",
    "    if log_exprs:\n",
    "        df_eng = df_eng.with_columns(log_exprs)\n",
    "    report[\"engineered_features_created\"].extend(log_cols_created)\n",
    "    print(f\"Created {len(log_cols_created)} log1p-transformed features.\")\n",
    "\n",
    "    # --- Quantile binning target rate plot (Polars \u2014 no .qcut) ---\n",
    "    bin_demo_cols = inferred_num[:4]\n",
    "    if bin_demo_cols and task_type in (\"binary\", \"multiclass\"):\n",
    "        ncols_p = min(2, len(bin_demo_cols))\n",
    "        nrows_p = (len(bin_demo_cols) + ncols_p - 1) // ncols_p\n",
    "        fig, axes = plt.subplots(nrows_p, ncols_p, figsize=(6 * ncols_p, 4 * nrows_p))\n",
    "        axes_flat = [axes] if len(bin_demo_cols) == 1 else list(np.array(axes).flatten())\n",
    "\n",
    "        for idx, c in enumerate(bin_demo_cols):\n",
    "            try:\n",
    "                # Compute decile boundaries in Polars\n",
    "                col_clean = df_eng.select([c, target_col]).drop_nulls()\n",
    "                boundaries = [col_clean[c].quantile(q / 10) for q in range(1, 10)]\n",
    "                boundaries = sorted(set(b for b in boundaries if b is not None))\n",
    "\n",
    "                if len(boundaries) < 2:\n",
    "                    axes_flat[idx].set_title(f\"{c} \u2014 too few unique quantiles\")\n",
    "                    continue\n",
    "\n",
    "                # Use .cut() with precomputed boundaries (no labels= arg)\n",
    "                binned = (\n",
    "                    col_clean.with_columns(\n",
    "                        pl.col(c).cut(boundaries).alias(\"bin\")\n",
    "                    )\n",
    "                    .group_by(\"bin\")\n",
    "                    .agg([\n",
    "                        pl.col(target_col).mean().alias(\"target_rate\"),\n",
    "                        pl.col(target_col).count().alias(\"n\"),\n",
    "                    ])\n",
    "                    .sort(\"bin\")\n",
    "                )\n",
    "                bin_labels = binned[\"bin\"].cast(pl.Utf8).to_list()\n",
    "                bin_rates = binned[\"target_rate\"].to_list()\n",
    "                axes_flat[idx].bar(range(len(bin_labels)), bin_rates, color=\"#8e44ad\", alpha=0.8)\n",
    "                axes_flat[idx].set_xticks(range(len(bin_labels)))\n",
    "                axes_flat[idx].set_xticklabels(bin_labels, fontsize=6, rotation=45, ha=\"right\")\n",
    "                axes_flat[idx].set_title(f\"Target Rate by Bin: {c}\", fontsize=9)\n",
    "                axes_flat[idx].set_ylabel(\"Target Rate\")\n",
    "            except Exception as e:\n",
    "                axes_flat[idx].set_title(f\"{c} \u2014 error: {e}\")\n",
    "        for j in range(idx + 1, len(axes_flat)):\n",
    "            axes_flat[j].set_visible(False)\n",
    "        fig.suptitle(\"Target Rate by Quantile Bins (numeric features)\", fontsize=12, y=1.01)\n",
    "        fig.tight_layout()\n",
    "        save_plot(fig, \"f2_quantile_bin_target_rate.png\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F.3 \u2014 Interaction Features\n",
    "\n",
    "For the top numeric features (by consensus), we create ratio and product\n",
    "interactions. Limited to prevent combinatorial explosion."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# F.3 \u2014 Interaction features (Polars)\n",
    "# ---------------------------------------------------------------------------\n",
    "interaction_cols_created = []\n",
    "\n",
    "if enable_interactions and inferred_num and consensus_sorted:\n",
    "    top_num_for_interact = [f for f, _ in consensus_sorted if f in inferred_num][:5]\n",
    "\n",
    "    if len(top_num_for_interact) >= 2:\n",
    "        interact_exprs = []\n",
    "        pairs_done = 0\n",
    "        max_pairs = 10\n",
    "        for i in range(len(top_num_for_interact)):\n",
    "            for j in range(i + 1, len(top_num_for_interact)):\n",
    "                if pairs_done >= max_pairs:\n",
    "                    break\n",
    "                a, b = top_num_for_interact[i], top_num_for_interact[j]\n",
    "                ratio_col = f\"{a}_div_{b}\"\n",
    "                prod_col = f\"{a}_x_{b}\"\n",
    "                interact_exprs.extend([\n",
    "                    (pl.col(a).fill_null(0) / (pl.col(b).fill_null(0).abs() + 1e-8)).alias(ratio_col),\n",
    "                    (pl.col(a).fill_null(0) * pl.col(b).fill_null(0)).alias(prod_col),\n",
    "                ])\n",
    "                interaction_cols_created.extend([ratio_col, prod_col])\n",
    "                pairs_done += 1\n",
    "\n",
    "        if interact_exprs:\n",
    "            df_eng = df_eng.with_columns(interact_exprs)\n",
    "        report[\"engineered_features_created\"].extend(interaction_cols_created)\n",
    "        print(f\"Created {len(interaction_cols_created)} interaction features.\")\n",
    "\n",
    "        # --- Plot: 2D scatter for first 3 pairs (binary only) ---\n",
    "        if task_type == \"binary\" and len(top_num_for_interact) >= 2:\n",
    "            pairs_to_plot = min(3, len(top_num_for_interact) * (len(top_num_for_interact) - 1) // 2)\n",
    "            fig, axes = plt.subplots(1, pairs_to_plot, figsize=(5 * pairs_to_plot, 4))\n",
    "            if pairs_to_plot == 1:\n",
    "                axes = [axes]\n",
    "            df_scatter = safe_sample(df_eng, 2000, sample_seed)\n",
    "            pair_idx = 0\n",
    "            for i in range(len(top_num_for_interact)):\n",
    "                for j in range(i + 1, len(top_num_for_interact)):\n",
    "                    if pair_idx >= pairs_to_plot:\n",
    "                        break\n",
    "                    a, b = top_num_for_interact[i], top_num_for_interact[j]\n",
    "                    scatter_df = df_scatter.select([a, b, target_col]).drop_nulls()\n",
    "                    axes[pair_idx].scatter(\n",
    "                        scatter_df[a].to_list(), scatter_df[b].to_list(),\n",
    "                        c=scatter_df[target_col].to_list(), cmap=\"coolwarm\", alpha=0.3, s=8,\n",
    "                    )\n",
    "                    axes[pair_idx].set_xlabel(a, fontsize=8)\n",
    "                    axes[pair_idx].set_ylabel(b, fontsize=8)\n",
    "                    axes[pair_idx].set_title(f\"{a} vs {b}\", fontsize=9)\n",
    "                    pair_idx += 1\n",
    "            fig.suptitle(\"Interaction Pairs \u2014 2D Scatter Colored by Target\", fontsize=12, y=1.02)\n",
    "            fig.tight_layout()\n",
    "            save_plot(fig, \"f3_interaction_scatter.png\")\n",
    "    else:\n",
    "        print(\"Not enough numeric features for interactions.\")\n",
    "elif enable_polynomial:\n",
    "    print(\"WARNING: enable_polynomial=True can create feature explosion.\")\n",
    "if not enable_interactions:\n",
    "    print(\"Interaction features disabled.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F.4 \u2014 Group Aggregation Features\n",
    "\n",
    "When `group_cols` are provided, we compute per-group statistics (mean, std,\n",
    "count) for numeric features in Polars."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# F.4 \u2014 Group aggregation features (Polars)\n",
    "# ---------------------------------------------------------------------------\n",
    "group_agg_cols_created = []\n",
    "\n",
    "if enable_group_aggregations and group_cols:\n",
    "    top_num_for_agg = [f for f, _ in consensus_sorted if f in inferred_num][:5]\n",
    "\n",
    "    for gc in group_cols:\n",
    "        if gc not in df_eng.columns:\n",
    "            continue\n",
    "        agg_exprs = []\n",
    "        new_names = []\n",
    "        for nc in top_num_for_agg:\n",
    "            mean_col = f\"{nc}_grp_{gc}_mean\"\n",
    "            std_col = f\"{nc}_grp_{gc}_std\"\n",
    "            agg_exprs.extend([\n",
    "                pl.col(nc).mean().alias(mean_col),\n",
    "                pl.col(nc).std().alias(std_col),\n",
    "            ])\n",
    "            new_names.extend([mean_col, std_col])\n",
    "\n",
    "        count_col = f\"grp_{gc}_count\"\n",
    "        agg_exprs.append(pl.len().alias(count_col))\n",
    "        new_names.append(count_col)\n",
    "\n",
    "        group_stats = df_eng.group_by(gc).agg(agg_exprs)\n",
    "        df_eng = df_eng.join(group_stats, on=gc, how=\"left\")\n",
    "        group_agg_cols_created.extend(new_names)\n",
    "\n",
    "    # Deduplicate\n",
    "    group_agg_cols_created = list(dict.fromkeys(group_agg_cols_created))\n",
    "    report[\"engineered_features_created\"].extend(group_agg_cols_created)\n",
    "    print(f\"Created {len(group_agg_cols_created)} group aggregation features.\")\n",
    "\n",
    "    # --- Plot ---\n",
    "    agg_to_plot = [c for c in group_agg_cols_created if c in df_eng.columns][:4]\n",
    "    if agg_to_plot:\n",
    "        fig, axes = plt.subplots(1, len(agg_to_plot), figsize=(5 * len(agg_to_plot), 4))\n",
    "        if len(agg_to_plot) == 1:\n",
    "            axes = [axes]\n",
    "        for idx, ac in enumerate(agg_to_plot):\n",
    "            vals = df_eng[ac].drop_nulls().to_list()\n",
    "            axes[idx].hist(vals, bins=40, color=\"#27ae60\", alpha=0.7, edgecolor=\"white\", linewidth=0.3)\n",
    "            axes[idx].set_title(ac, fontsize=9)\n",
    "        fig.suptitle(\"Group Aggregation Feature Distributions\", fontsize=12, y=1.02)\n",
    "        fig.tight_layout()\n",
    "        save_plot(fig, \"f4_group_agg_distributions.png\")\n",
    "else:\n",
    "    if not group_cols:\n",
    "        print(\"No group_cols \u2014 skipping group aggregations.\")\n",
    "    else:\n",
    "        print(\"Group aggregations disabled.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F.5 \u2014 Time-Based Features\n",
    "\n",
    "When a `time_col` is provided, we extract calendar components and their\n",
    "cyclic (sin/cos) encodings in Polars."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# F.5 \u2014 Time features (Polars)\n",
    "# ---------------------------------------------------------------------------\n",
    "time_cols_created = []\n",
    "\n",
    "if enable_time_features and time_col and time_col in df_eng.columns:\n",
    "    if df_eng[time_col].dtype not in (pl.Date, pl.Datetime):\n",
    "        df_eng = df_eng.with_columns(pl.col(time_col).cast(pl.Date))\n",
    "\n",
    "    pi2 = 2 * math.pi\n",
    "    df_eng = df_eng.with_columns([\n",
    "        pl.col(time_col).dt.weekday().alias(\"dow\"),\n",
    "        pl.col(time_col).dt.month().alias(\"month_num\"),\n",
    "        (pi2 * pl.col(time_col).dt.weekday() / 7).sin().alias(\"dow_sin\"),\n",
    "        (pi2 * pl.col(time_col).dt.weekday() / 7).cos().alias(\"dow_cos\"),\n",
    "        (pi2 * pl.col(time_col).dt.month() / 12).sin().alias(\"month_sin\"),\n",
    "        (pi2 * pl.col(time_col).dt.month() / 12).cos().alias(\"month_cos\"),\n",
    "        (pl.col(time_col).dt.weekday() >= 6).cast(pl.Int8).alias(\"is_weekend\"),\n",
    "    ])\n",
    "\n",
    "    time_cols_created = [\"dow\", \"month_num\", \"dow_sin\", \"dow_cos\",\n",
    "                         \"month_sin\", \"month_cos\", \"is_weekend\"]\n",
    "    report[\"engineered_features_created\"].extend(time_cols_created)\n",
    "    print(f\"Created {len(time_cols_created)} time features from {time_col}.\")\n",
    "\n",
    "    # --- Plot ---\n",
    "    if task_type in (\"binary\", \"multiclass\"):\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "        dow_agg = (\n",
    "            df_eng.group_by(\"dow\")\n",
    "            .agg(pl.col(target_col).mean().alias(\"target_rate\"))\n",
    "            .sort(\"dow\")\n",
    "        )\n",
    "        dow_labels = [\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"]\n",
    "        ax1.bar(dow_labels[:len(dow_agg)], dow_agg[\"target_rate\"].to_list(), color=\"#3498db\", alpha=0.8)\n",
    "        ax1.set_title(\"Target Rate by Day of Week\")\n",
    "        ax1.set_ylabel(\"Target Rate\")\n",
    "\n",
    "        month_agg = (\n",
    "            df_eng.group_by(\"month_num\")\n",
    "            .agg(pl.col(target_col).mean().alias(\"target_rate\"))\n",
    "            .sort(\"month_num\")\n",
    "        )\n",
    "        month_labels = [\"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\",\"Jun\",\"Jul\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\"]\n",
    "        m_indices = month_agg[\"month_num\"].to_list()\n",
    "        m_labels = [month_labels[m-1] if 1 <= m <= 12 else str(m) for m in m_indices]\n",
    "        ax2.bar(m_labels, month_agg[\"target_rate\"].to_list(), color=\"#e67e22\", alpha=0.8)\n",
    "        ax2.set_title(\"Target Rate by Month\")\n",
    "        ax2.set_ylabel(\"Target Rate\")\n",
    "        ax2.tick_params(axis=\"x\", rotation=45)\n",
    "        fig.suptitle(\"Time-Based Target Patterns\", fontsize=12, y=1.02)\n",
    "        fig.tight_layout()\n",
    "        save_plot(fig, \"f5_time_target_patterns.png\")\n",
    "else:\n",
    "    if not time_col:\n",
    "        print(\"No time_col \u2014 skipping time features.\")\n",
    "    else:\n",
    "        print(\"Time features disabled.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F.6 \u2014 Categorical Handling\n",
    "\n",
    "1. **Rare category bucketing** \u2014 merge rare categories into `__RARE__`.\n",
    "2. **Frequency encoding** \u2014 replace categories with their log-count.\n",
    "3. **Target encoding** (optional) \u2014 cross-validated to prevent leakage."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# F.6 \u2014 Categorical handling (Polars)\n",
    "# ---------------------------------------------------------------------------\n",
    "freq_enc_cols = []\n",
    "\n",
    "if inferred_cat:\n",
    "    # Rare category bucketing in Polars\n",
    "    for c in inferred_cat:\n",
    "        vc = df_eng[c].cast(pl.Utf8).fill_null(\"__NULL__\").value_counts()\n",
    "        rare_vals = vc.filter(pl.col(\"count\") < rare_category_min_count)[c].to_list()\n",
    "        if rare_vals:\n",
    "            df_eng = df_eng.with_columns(\n",
    "                pl.when(pl.col(c).cast(pl.Utf8).is_in(rare_vals))\n",
    "                .then(pl.lit(\"__RARE__\"))\n",
    "                .otherwise(pl.col(c).cast(pl.Utf8))\n",
    "                .alias(c)\n",
    "            )\n",
    "    print(f\"Rare category bucketing applied (threshold={rare_category_min_count}).\")\n",
    "\n",
    "    # Frequency encoding in Polars (log of count)\n",
    "    for c in inferred_cat:\n",
    "        freq_col = f\"{c}_freq_enc\"\n",
    "        freq_map = df_eng.group_by(c).agg(pl.len().alias(\"_freq\"))\n",
    "        df_eng = df_eng.join(freq_map, on=c, how=\"left\")\n",
    "        df_eng = df_eng.with_columns(\n",
    "            pl.col(\"_freq\").cast(pl.Float64).log1p().alias(freq_col)\n",
    "        ).drop(\"_freq\")\n",
    "        freq_enc_cols.append(freq_col)\n",
    "\n",
    "    report[\"engineered_features_created\"].extend(freq_enc_cols)\n",
    "    print(f\"Created {len(freq_enc_cols)} frequency-encoded features.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# F.6b \u2014 Cross-validated target encoding (Polars-first)\n",
    "# ---------------------------------------------------------------------------\n",
    "target_enc_cols = []\n",
    "\n",
    "if enable_target_encoding and inferred_cat:\n",
    "    print(\"Target encoding enabled \u2014 using K-fold cross-validation to avoid leakage.\")\n",
    "    print(\"IMPORTANT: Without cross-validation, target encoding leaks the target\")\n",
    "    print(\"into features and inflates model performance.\\n\")\n",
    "\n",
    "    global_mean = df_eng[target_col].mean()\n",
    "\n",
    "    for c in inferred_cat[:10]:\n",
    "        te_col = f\"{c}_target_enc\"\n",
    "        # Add a fold column for CV\n",
    "        n_rows = len(df_eng)\n",
    "        fold_assignments = [(i % cv_folds) for i in range(n_rows)]\n",
    "        df_eng = df_eng.with_columns(pl.Series(\"_fold\", fold_assignments))\n",
    "\n",
    "        # Initialize target encoding column with global mean\n",
    "        df_eng = df_eng.with_columns(pl.lit(global_mean).alias(te_col))\n",
    "\n",
    "        cat_col_str = df_eng[c].cast(pl.Utf8).fill_null(\"__NULL__\")\n",
    "        df_eng = df_eng.with_columns(cat_col_str.alias(\"_cat_str\"))\n",
    "\n",
    "        for fold in range(cv_folds):\n",
    "            # Compute target means on training folds\n",
    "            train_means = (\n",
    "                df_eng.filter(pl.col(\"_fold\") != fold)\n",
    "                .group_by(\"_cat_str\")\n",
    "                .agg(pl.col(target_col).mean().alias(\"_te_mean\"))\n",
    "            )\n",
    "            # Apply to validation fold via join\n",
    "            df_eng = df_eng.join(train_means, on=\"_cat_str\", how=\"left\")\n",
    "            df_eng = df_eng.with_columns(\n",
    "                pl.when(pl.col(\"_fold\") == fold)\n",
    "                .then(pl.col(\"_te_mean\").fill_null(global_mean))\n",
    "                .otherwise(pl.col(te_col))\n",
    "                .alias(te_col)\n",
    "            ).drop(\"_te_mean\")\n",
    "\n",
    "        df_eng = df_eng.drop([\"_fold\", \"_cat_str\"])\n",
    "        target_enc_cols.append(te_col)\n",
    "\n",
    "    report[\"engineered_features_created\"].extend(target_enc_cols)\n",
    "    print(f\"Created {len(target_enc_cols)} target-encoded features (CV'd).\")\n",
    "\n",
    "    # --- Plot ---\n",
    "    if target_enc_cols and task_type in (\"binary\", \"multiclass\"):\n",
    "        n_to_show = min(3, len(target_enc_cols))\n",
    "        fig, axes = plt.subplots(1, n_to_show, figsize=(5 * n_to_show, 4))\n",
    "        if n_to_show == 1:\n",
    "            axes = [axes]\n",
    "        df_te_sample = safe_sample(df_eng, 3000, sample_seed)\n",
    "        for i, tc in enumerate(target_enc_cols[:n_to_show]):\n",
    "            te_vals = df_te_sample[tc].to_list()\n",
    "            t_vals = df_te_sample[target_col].to_list()\n",
    "            # Add jitter via Python random\n",
    "            t_jittered = [tv + random.gauss(0, 0.05) for tv in t_vals]\n",
    "            axes[i].scatter(te_vals, t_jittered, alpha=0.2, s=5, color=\"#e74c3c\")\n",
    "            axes[i].set_xlabel(tc, fontsize=8)\n",
    "            axes[i].set_ylabel(\"Target (jittered)\")\n",
    "            axes[i].set_title(tc.replace(\"_target_enc\", \"\"), fontsize=9)\n",
    "        fig.suptitle(\"Target Encoding vs Actual Target\", fontsize=12, y=1.02)\n",
    "        fig.tight_layout()\n",
    "        save_plot(fig, \"f6_target_encoding.png\")\n",
    "elif enable_target_encoding:\n",
    "    print(\"Target encoding enabled but no categorical features found.\")\n",
    "else:\n",
    "    print(\"Target encoding disabled.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F.7 \u2014 Text Features (lightweight)\n",
    "\n",
    "If `text_cols` are provided, we extract simple statistics in Polars:\n",
    "character length, word count, digit count."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# F.7 \u2014 Text features (Polars)\n",
    "# ---------------------------------------------------------------------------\n",
    "text_feat_cols = []\n",
    "\n",
    "if text_cols:\n",
    "    text_exprs = []\n",
    "    for tc in text_cols:\n",
    "        if tc not in df_eng.columns:\n",
    "            continue\n",
    "        len_col = f\"{tc}_len\"\n",
    "        wc_col = f\"{tc}_word_count\"\n",
    "        digit_col = f\"{tc}_digit_count\"\n",
    "        text_exprs.extend([\n",
    "            pl.col(tc).cast(pl.Utf8).fill_null(\"\").str.len_chars().alias(len_col),\n",
    "            pl.col(tc).cast(pl.Utf8).fill_null(\"\").str.split(\" \").list.len().alias(wc_col),\n",
    "            pl.col(tc).cast(pl.Utf8).fill_null(\"\").str.count_matches(r\"\\d\").alias(digit_col),\n",
    "        ])\n",
    "        text_feat_cols.extend([len_col, wc_col, digit_col])\n",
    "\n",
    "    if text_exprs:\n",
    "        df_eng = df_eng.with_columns(text_exprs)\n",
    "    report[\"engineered_features_created\"].extend(text_feat_cols)\n",
    "    print(f\"Created {len(text_feat_cols)} text features from {len(text_cols)} text columns.\")\n",
    "\n",
    "    # --- Plot: text length vs target ---\n",
    "    if text_feat_cols and task_type in (\"binary\", \"multiclass\"):\n",
    "        len_cols_to_plot = [c for c in text_feat_cols if c.endswith(\"_len\")][:3]\n",
    "        if len_cols_to_plot:\n",
    "            fig, axes = plt.subplots(1, len(len_cols_to_plot), figsize=(5 * len(len_cols_to_plot), 4))\n",
    "            if len(len_cols_to_plot) == 1:\n",
    "                axes = [axes]\n",
    "            target_classes = sorted(df_eng[target_col].unique().to_list())\n",
    "            for i, lc in enumerate(len_cols_to_plot):\n",
    "                for cls_val in target_classes:\n",
    "                    subset = df_eng.filter(pl.col(target_col) == cls_val)\n",
    "                    vals = subset[lc].drop_nulls().to_list()\n",
    "                    axes[i].hist(vals, bins=30, alpha=0.5, label=f\"class={cls_val}\",\n",
    "                               edgecolor=\"white\", linewidth=0.3)\n",
    "                axes[i].set_title(lc, fontsize=9)\n",
    "                axes[i].set_xlabel(\"Length\")\n",
    "                axes[i].legend(fontsize=7)\n",
    "            fig.suptitle(\"Text Length Distribution by Target Class\", fontsize=12, y=1.02)\n",
    "            fig.tight_layout()\n",
    "            save_plot(fig, \"f7_text_length_by_target.png\")\n",
    "else:\n",
    "    print(\"No text columns \u2014 skipping text features.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## G \u2014 Model-Based Feature Selection\n",
    "\n",
    "We use a simple baseline model purely for feature importance and selection.\n",
    "\n",
    "> **sklearn boundary:** Data is prepared in Polars (fill nulls, cast types),\n",
    "> then converted to numpy once via `polars_to_numpy_for_sklearn()`.\n",
    "\n",
    "1. **L1-regularized selection** \u2014 drives unimportant coefficients to zero\n",
    "2. **Permutation importance** \u2014 model-agnostic, measures actual predictive lift\n",
    "3. **Stability selection** \u2014 consistency across CV folds\n",
    "4. **Performance vs number of features** curve"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# G \u2014 Prepare train/test split (Polars \u2192 numpy at sklearn boundary)\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "# Collect all numeric feature columns (original + engineered)\n",
    "all_eng_features = list(dict.fromkeys(\n",
    "    inferred_num\n",
    "    + freq_enc_cols\n",
    "    + log_cols_created\n",
    "    + interaction_cols_created\n",
    "    + group_agg_cols_created\n",
    "    + time_cols_created\n",
    "    + text_feat_cols\n",
    "    + target_enc_cols\n",
    "    + miss_indicator_cols\n",
    "))\n",
    "# Filter to columns that exist and are numeric\n",
    "all_eng_features = [\n",
    "    c for c in all_eng_features\n",
    "    if c in df_eng.columns and df_eng[c].dtype.is_numeric()\n",
    "]\n",
    "print(f\"Total numeric features for model-based selection: {len(all_eng_features)}\")\n",
    "\n",
    "# Convert to numpy at sklearn boundary\n",
    "X_all, y_all = polars_to_numpy_for_sklearn(df_eng, all_eng_features, target_col)\n",
    "\n",
    "# Train/test split\n",
    "if task_type in (\"binary\", \"multiclass\") and stratify:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_all, y_all, test_size=test_size, random_state=random_state, stratify=y_all,\n",
    "    )\n",
    "else:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_all, y_all, test_size=test_size, random_state=random_state,\n",
    "    )\n",
    "print(f\"Train shape: {X_train.shape}, Test shape: {X_test.shape}\")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_sc = scaler.fit_transform(X_train)\n",
    "X_test_sc = scaler.transform(X_test)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### G.1 \u2014 L1-Regularized Feature Selection\n",
    "\n",
    "L1 (Lasso) regularization drives unimportant coefficients to zero. Features\n",
    "with non-zero coefficients are \"selected\"."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# G.1 \u2014 L1-regularized selection\n",
    "# ---------------------------------------------------------------------------\n",
    "l1_selected = []\n",
    "\n",
    "if task_type in (\"binary\", \"multiclass\"):\n",
    "    l1_model = LogisticRegression(\n",
    "        penalty=\"l1\", solver=\"liblinear\", C=1.0,\n",
    "        max_iter=1000, random_state=random_state,\n",
    "    )\n",
    "    l1_model.fit(X_train_sc, y_train)\n",
    "    coef = abs(l1_model.coef_).mean(axis=0) if l1_model.coef_.ndim > 1 else abs(l1_model.coef_[0])\n",
    "else:\n",
    "    from sklearn.linear_model import Lasso\n",
    "    l1_model = Lasso(alpha=0.01, max_iter=2000, random_state=random_state)\n",
    "    l1_model.fit(X_train_sc, y_train)\n",
    "    coef = abs(l1_model.coef_)\n",
    "\n",
    "l1_importance = {all_eng_features[i]: float(coef[i]) for i in range(len(all_eng_features))}\n",
    "l1_selected = [f for f, v in l1_importance.items() if v > 1e-6]\n",
    "print(f\"L1 selection: {len(l1_selected)} / {len(all_eng_features)} features with non-zero coefficient.\")\n",
    "\n",
    "report[\"model_based_importance\"][\"l1_coefficients\"] = sorted(\n",
    "    l1_importance.items(), key=lambda x: x[1], reverse=True\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### G.2 \u2014 Permutation Importance\n",
    "\n",
    "Measures how much the model's score drops when a feature is shuffled."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# G.2 \u2014 Permutation importance\n",
    "# ---------------------------------------------------------------------------\n",
    "perm_importance_dict = {}\n",
    "\n",
    "if enable_permutation_importance:\n",
    "    if task_type in (\"binary\", \"multiclass\"):\n",
    "        base_model = LogisticRegression(max_iter=1000, random_state=random_state, solver=\"lbfgs\")\n",
    "        scoring_str = \"f1_macro\" if task_type == \"multiclass\" else \"f1\"\n",
    "    else:\n",
    "        base_model = Ridge(alpha=1.0, random_state=random_state)\n",
    "        scoring_str = \"neg_root_mean_squared_error\"\n",
    "\n",
    "    base_model.fit(X_train_sc, y_train)\n",
    "    perm_result = permutation_importance(\n",
    "        base_model, X_test_sc, y_test,\n",
    "        n_repeats=10, random_state=random_state,\n",
    "        scoring=scoring_str, n_jobs=-1,\n",
    "    )\n",
    "    perm_importance_dict = {\n",
    "        all_eng_features[i]: float(perm_result.importances_mean[i])\n",
    "        for i in range(len(all_eng_features))\n",
    "    }\n",
    "    perm_sorted = sorted(perm_importance_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "    report[\"model_based_importance\"][\"permutation_importance\"] = [\n",
    "        {\"feature\": f, \"importance\": round(v, 6)} for f, v in perm_sorted\n",
    "    ]\n",
    "\n",
    "    # --- Plot ---\n",
    "    top_perm = perm_sorted[:30]\n",
    "    fig, ax = plt.subplots(figsize=(10, max(5, len(top_perm) * 0.25)))\n",
    "    ax.barh(range(len(top_perm)), [v for _, v in top_perm], color=\"#c0392b\", alpha=0.8)\n",
    "    ax.set_yticks(range(len(top_perm)))\n",
    "    ax.set_yticklabels([f for f, _ in top_perm], fontsize=8)\n",
    "    ax.set_xlabel(\"Mean Permutation Importance\")\n",
    "    ax.set_title(\"Top 30 Features \u2014 Permutation Importance\")\n",
    "    ax.invert_yaxis()\n",
    "    fig.tight_layout()\n",
    "    save_plot(fig, \"g2_permutation_importance.png\")\n",
    "    print(f\"Permutation importance computed for {len(perm_importance_dict)} features.\")\n",
    "else:\n",
    "    print(\"Permutation importance disabled.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### G.3 \u2014 Stability Selection (Selection Frequency Across CV Folds)\n",
    "\n",
    "How consistently is each feature selected across different folds?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# G.3 \u2014 Stability selection\n",
    "# ---------------------------------------------------------------------------\n",
    "selection_freq: dict[str, int] = {f: 0 for f in all_eng_features}\n",
    "\n",
    "if task_type in (\"binary\", \"multiclass\"):\n",
    "    kf = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=random_state)\n",
    "    split_iter = list(kf.split(X_train_sc, y_train))\n",
    "else:\n",
    "    kf = KFold(n_splits=cv_folds, shuffle=True, random_state=random_state)\n",
    "    split_iter = list(kf.split(X_train_sc))\n",
    "\n",
    "for fold_idx, (tr_idx, va_idx) in enumerate(split_iter):\n",
    "    X_fold_tr, y_fold_tr = X_train_sc[tr_idx], y_train[tr_idx]\n",
    "    if task_type in (\"binary\", \"multiclass\"):\n",
    "        fold_model = LogisticRegression(\n",
    "            penalty=\"l1\", solver=\"liblinear\", C=1.0,\n",
    "            max_iter=1000, random_state=random_state,\n",
    "        )\n",
    "    else:\n",
    "        from sklearn.linear_model import Lasso\n",
    "        fold_model = Lasso(alpha=0.01, max_iter=2000, random_state=random_state)\n",
    "    fold_model.fit(X_fold_tr, y_fold_tr)\n",
    "\n",
    "    if hasattr(fold_model, \"coef_\"):\n",
    "        fc = abs(fold_model.coef_)\n",
    "        if fc.ndim > 1:\n",
    "            fc = fc.mean(axis=0)\n",
    "        for i, f in enumerate(all_eng_features):\n",
    "            if fc[i] > 1e-6:\n",
    "                selection_freq[f] += 1\n",
    "\n",
    "freq_sorted = sorted(selection_freq.items(), key=lambda x: x[1], reverse=True)\n",
    "report[\"selection_frequency\"] = {f: c for f, c in freq_sorted}\n",
    "\n",
    "top_freq = [(f, c) for f, c in freq_sorted if c > 0][:40]\n",
    "if top_freq:\n",
    "    fig, ax = plt.subplots(figsize=(10, max(5, len(top_freq) * 0.25)))\n",
    "    ax.barh(range(len(top_freq)), [c for _, c in top_freq], color=\"#2980b9\", alpha=0.8)\n",
    "    ax.set_yticks(range(len(top_freq)))\n",
    "    ax.set_yticklabels([f for f, _ in top_freq], fontsize=7)\n",
    "    ax.set_xlabel(f\"Selection Count (out of {cv_folds} folds)\")\n",
    "    ax.set_title(f\"Feature Selection Frequency Across {cv_folds} CV Folds\")\n",
    "    ax.set_xlim(0, cv_folds + 0.5)\n",
    "    ax.invert_yaxis()\n",
    "    fig.tight_layout()\n",
    "    save_plot(fig, \"g3_selection_frequency.png\")\n",
    "    print(f\"Features in all {cv_folds} folds: {sum(1 for _, c in freq_sorted if c == cv_folds)}\")\n",
    "else:\n",
    "    print(\"No features selected in any fold.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### G.4 \u2014 Performance vs Number of Features\n",
    "\n",
    "Evaluate the baseline model using top-k features for increasing k. The\n",
    "\"elbow plot\" identifies the point of diminishing returns."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# G.4 \u2014 Performance vs number of features\n",
    "# ---------------------------------------------------------------------------\n",
    "if perm_importance_dict:\n",
    "    ranked_feats = [f for f, _ in sorted(perm_importance_dict.items(), key=lambda x: x[1], reverse=True)]\n",
    "elif consensus_sorted:\n",
    "    ranked_feats = [f for f, _ in consensus_sorted if f in all_eng_features]\n",
    "else:\n",
    "    ranked_feats = all_eng_features\n",
    "\n",
    "k_values = sorted({k for k in [5, 10, 15, 20, 30, 50, 75, 100, len(ranked_feats)]\n",
    "                    if 0 < k <= len(ranked_feats)})\n",
    "\n",
    "topk_results = []\n",
    "for k in k_values:\n",
    "    top_k_feats = ranked_feats[:k]\n",
    "    feat_indices = [all_eng_features.index(f) for f in top_k_feats if f in all_eng_features]\n",
    "    if not feat_indices:\n",
    "        continue\n",
    "    X_tr_k = X_train_sc[:, feat_indices]\n",
    "    X_te_k = X_test_sc[:, feat_indices]\n",
    "\n",
    "    if task_type in (\"binary\", \"multiclass\"):\n",
    "        m = LogisticRegression(max_iter=1000, random_state=random_state, solver=\"lbfgs\")\n",
    "    else:\n",
    "        m = Ridge(alpha=1.0, random_state=random_state)\n",
    "    m.fit(X_tr_k, y_train)\n",
    "    y_pred = m.predict(X_te_k)\n",
    "\n",
    "    if task_type == \"binary\":\n",
    "        score = f1_score(y_test, y_pred, zero_division=0)\n",
    "        metric_name = \"F1\"\n",
    "    elif task_type == \"multiclass\":\n",
    "        score = f1_score(y_test, y_pred, average=\"macro\", zero_division=0)\n",
    "        metric_name = \"F1 (macro)\"\n",
    "    else:\n",
    "        score = -math.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        metric_name = \"Neg RMSE\"\n",
    "\n",
    "    topk_results.append({\"k\": k, \"metric\": metric_name, \"score\": round(float(score), 4)})\n",
    "\n",
    "report[\"top_k_performance\"] = topk_results\n",
    "\n",
    "if topk_results:\n",
    "    ks = [r[\"k\"] for r in topk_results]\n",
    "    scores = [r[\"score\"] for r in topk_results]\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    ax.plot(ks, scores, marker=\"o\", color=\"#8e44ad\", linewidth=2, markersize=6)\n",
    "    ax.fill_between(ks, scores, alpha=0.1, color=\"#8e44ad\")\n",
    "    ax.set_xlabel(\"Number of Features (k)\")\n",
    "    ax.set_ylabel(topk_results[0][\"metric\"])\n",
    "    ax.set_title(f\"Performance vs Number of Features ({topk_results[0]['metric']})\")\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    best_idx = max(range(len(scores)), key=lambda i: scores[i])\n",
    "    ax.annotate(\n",
    "        f\"k={ks[best_idx]}, score={scores[best_idx]:.4f}\",\n",
    "        xy=(ks[best_idx], scores[best_idx]),\n",
    "        xytext=(ks[best_idx] + 2, scores[best_idx] - 0.01),\n",
    "        fontsize=9, color=\"#c0392b\",\n",
    "        arrowprops=dict(arrowstyle=\"->\", color=\"#c0392b\"),\n",
    "    )\n",
    "    fig.tight_layout()\n",
    "    save_plot(fig, \"g4_performance_vs_features.png\")\n",
    "    print(f\"Best performance at k={ks[best_idx]} features: {scores[best_idx]:.4f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## H \u2014 Leakage & Drift Checks\n",
    "\n",
    "### Leakage heuristics\n",
    "- Near-perfect univariate AUC (>0.95) \u2014 may encode the target directly.\n",
    "- High-cardinality + high correlation \u2014 ID-like columns disguised as features.\n",
    "\n",
    "### Drift checks (if `time_col` exists)\n",
    "- KS-test across time halves for top numeric features.\n",
    "- Compare chronological vs random split performance."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# H \u2014 Leakage heuristics\n",
    "# ---------------------------------------------------------------------------\n",
    "leakage_suspects = []\n",
    "\n",
    "if task_type == \"binary\" and \"univariate_auc\" in score_rankings:\n",
    "    for f, auc in score_rankings[\"univariate_auc\"].items():\n",
    "        if auc > 0.95:\n",
    "            leakage_suspects.append({\"feature\": f, \"reason\": f\"univariate_AUC={auc:.3f} (>0.95)\"})\n",
    "\n",
    "for f in feature_candidates:\n",
    "    nunique = df_raw[f].n_unique()\n",
    "    corr_val = score_rankings.get(\"correlation\", {}).get(f, 0)\n",
    "    mi_val = score_rankings.get(\"mutual_info\", {}).get(f, 0)\n",
    "    if nunique > high_cardinality_threshold and (corr_val > 0.5 or mi_val > 0.5):\n",
    "        leakage_suspects.append({\n",
    "            \"feature\": f,\n",
    "            \"reason\": f\"high_cardinality({nunique}) + high_signal(corr={corr_val:.2f}, MI={mi_val:.2f})\"\n",
    "        })\n",
    "\n",
    "report[\"high_risk_features\"].extend(leakage_suspects)\n",
    "if leakage_suspects:\n",
    "    print(\"Leakage suspects:\")\n",
    "    for s in leakage_suspects:\n",
    "        print(f\"  {s['feature']}: {s['reason']}\")\n",
    "else:\n",
    "    print(\"No leakage suspects identified.\")\n",
    "\n",
    "# --- Plot: n_unique vs correlation/MI bubble ---\n",
    "if inferred_num and \"correlation\" in score_rankings:\n",
    "    plot_feats = inferred_num[:50]\n",
    "    n_uniques = [df_raw[f].n_unique() for f in plot_feats]\n",
    "    corrs = [score_rankings[\"correlation\"].get(f, 0) for f in plot_feats]\n",
    "    mi_vals = [score_rankings.get(\"mutual_info\", {}).get(f, 0) for f in plot_feats]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    scatter = ax.scatter(\n",
    "        n_uniques, corrs,\n",
    "        s=[max(10, v * 500) for v in mi_vals],\n",
    "        c=mi_vals, cmap=\"YlOrRd\", alpha=0.7, edgecolors=\"black\", linewidth=0.5,\n",
    "    )\n",
    "    cbar = plt.colorbar(scatter, ax=ax)\n",
    "    cbar.set_label(\"Mutual Information\", fontsize=9)\n",
    "    ax.set_xlabel(\"Number of Unique Values\")\n",
    "    ax.set_ylabel(\"|Correlation with Target|\")\n",
    "    ax.set_title(\"Leakage Detection: n_unique vs Correlation (bubble = MI)\")\n",
    "    ax.set_xscale(\"log\")\n",
    "    for s in leakage_suspects:\n",
    "        f = s[\"feature\"]\n",
    "        if f in plot_feats:\n",
    "            idx = plot_feats.index(f)\n",
    "            ax.annotate(f, (n_uniques[idx], corrs[idx]), fontsize=7, color=\"red\")\n",
    "    fig.tight_layout()\n",
    "    save_plot(fig, \"h_leakage_bubble.png\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# H \u2014 Drift checks (Polars + scipy at KS-test boundary)\n",
    "# ---------------------------------------------------------------------------\n",
    "if time_col and time_col in df_raw.columns and enable_stability_checks:\n",
    "    print(\"Running feature drift checks across time periods...\\n\")\n",
    "\n",
    "    df_with_time = df_raw.filter(pl.col(time_col).is_not_null())\n",
    "    median_time = df_with_time[time_col].cast(pl.Date).median()\n",
    "    df_early = df_with_time.filter(pl.col(time_col).cast(pl.Date) <= median_time)\n",
    "    df_late = df_with_time.filter(pl.col(time_col).cast(pl.Date) > median_time)\n",
    "    print(f\"Early period: {len(df_early):,} rows, Late period: {len(df_late):,} rows\")\n",
    "\n",
    "    # KS-test for top numeric features (scipy \u2014 no Polars equivalent)\n",
    "    drift_results = []\n",
    "    for f in inferred_num[:20]:\n",
    "        early_vals = df_early[f].drop_nulls().to_list()\n",
    "        late_vals = df_late[f].drop_nulls().to_list()\n",
    "        if len(early_vals) < 10 or len(late_vals) < 10:\n",
    "            continue\n",
    "        ks_stat, ks_pval = sp_stats.ks_2samp(early_vals, late_vals)\n",
    "        drift_results.append({\n",
    "            \"feature\": f,\n",
    "            \"ks_statistic\": round(float(ks_stat), 4),\n",
    "            \"ks_pvalue\": round(float(ks_pval), 6),\n",
    "            \"drifted\": ks_pval < 0.01,\n",
    "        })\n",
    "\n",
    "    drift_df = pl.DataFrame(drift_results).sort(\"ks_statistic\", descending=True)\n",
    "    print(\"Feature drift (KS-test, early vs late period):\")\n",
    "    print(drift_df)\n",
    "\n",
    "    drifted_feats = [r[\"feature\"] for r in drift_results if r[\"drifted\"]]\n",
    "    for f in drifted_feats:\n",
    "        report[\"high_risk_features\"].append({\"feature\": f, \"reason\": \"temporal_drift (KS p<0.01)\"})\n",
    "\n",
    "    # --- Plot: KS statistic bar chart ---\n",
    "    dr_sorted = sorted(drift_results, key=lambda x: x[\"ks_statistic\"], reverse=True)\n",
    "    feats_dr = [r[\"feature\"] for r in dr_sorted]\n",
    "    ks_vals = [r[\"ks_statistic\"] for r in dr_sorted]\n",
    "    colors_dr = [\"#e74c3c\" if r[\"drifted\"] else \"#3498db\" for r in dr_sorted]\n",
    "    fig, ax = plt.subplots(figsize=(10, max(4, len(feats_dr) * 0.25)))\n",
    "    ax.barh(range(len(feats_dr)), ks_vals, color=colors_dr, alpha=0.8)\n",
    "    ax.set_yticks(range(len(feats_dr)))\n",
    "    ax.set_yticklabels(feats_dr, fontsize=8)\n",
    "    ax.set_xlabel(\"KS Statistic\")\n",
    "    ax.set_title(\"Feature Drift: KS-Test (red = significant drift at p<0.01)\")\n",
    "    ax.invert_yaxis()\n",
    "    fig.tight_layout()\n",
    "    save_plot(fig, \"h_feature_drift_ks.png\")\n",
    "\n",
    "    # --- Chronological split performance comparison ---\n",
    "    if all_eng_features and len(df_early) > 100 and len(df_late) > 100:\n",
    "        # Only use features that exist in df_early (original columns, not engineered)\n",
    "        early_cols = set(df_early.columns)\n",
    "        top_feats_chrono = [f for f in ranked_feats if f in early_cols][:min(30, len(ranked_feats))]\n",
    "        if not top_feats_chrono:\n",
    "            top_feats_chrono = [f for f in inferred_num if f in early_cols][:20]\n",
    "        X_early, y_early = polars_to_numpy_for_sklearn(df_early, top_feats_chrono, target_col)\n",
    "        X_late, y_late = polars_to_numpy_for_sklearn(df_late, top_feats_chrono, target_col)\n",
    "        sc2 = StandardScaler()\n",
    "        X_early_sc = sc2.fit_transform(X_early)\n",
    "        X_late_sc = sc2.transform(X_late)\n",
    "\n",
    "        if task_type in (\"binary\", \"multiclass\"):\n",
    "            chrono_model = LogisticRegression(max_iter=1000, random_state=random_state)\n",
    "        else:\n",
    "            chrono_model = Ridge(alpha=1.0, random_state=random_state)\n",
    "        chrono_model.fit(X_early_sc, y_early)\n",
    "        y_pred_chrono = chrono_model.predict(X_late_sc)\n",
    "\n",
    "        if task_type == \"binary\":\n",
    "            chrono_score = f1_score(y_late, y_pred_chrono, zero_division=0)\n",
    "            print(f\"\\nChronological split F1: {chrono_score:.4f}\")\n",
    "        elif task_type == \"multiclass\":\n",
    "            chrono_score = f1_score(y_late, y_pred_chrono, average=\"macro\", zero_division=0)\n",
    "            print(f\"\\nChronological split F1 (macro): {chrono_score:.4f}\")\n",
    "        else:\n",
    "            chrono_score = math.sqrt(mean_squared_error(y_late, y_pred_chrono))\n",
    "            print(f\"\\nChronological split RMSE: {chrono_score:.4f}\")\n",
    "\n",
    "        if topk_results:\n",
    "            random_score = max(r[\"score\"] for r in topk_results)\n",
    "            print(f\"Random split best score: {random_score:.4f}\")\n",
    "            fig, ax = plt.subplots(figsize=(6, 4))\n",
    "            ax.bar([\"Random Split\", \"Chronological Split\"],\n",
    "                   [random_score, chrono_score], color=[\"#3498db\", \"#e67e22\"], alpha=0.8)\n",
    "            ax.set_ylabel(\"Score\")\n",
    "            ax.set_title(\"Random vs Chronological Split Performance\")\n",
    "            for i, v in enumerate([random_score, chrono_score]):\n",
    "                ax.text(i, v, f\"{v:.4f}\", ha=\"center\", va=\"bottom\", fontsize=10)\n",
    "            fig.tight_layout()\n",
    "            save_plot(fig, \"h_chrono_vs_random.png\")\n",
    "else:\n",
    "    if not time_col:\n",
    "        print(\"No time_col \u2014 skipping drift checks.\")\n",
    "    else:\n",
    "        print(\"Stability checks disabled.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## I \u2014 Final Recommendations & Exports\n",
    "\n",
    "Combining all signals from univariate scoring, model-based importance,\n",
    "stability selection, and leakage checks."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# I \u2014 Final recommended feature set\n",
    "# ---------------------------------------------------------------------------\n",
    "stable_features = [f for f, c in selection_freq.items() if c >= max(1, cv_folds - 1)]\n",
    "if perm_importance_dict:\n",
    "    perm_positive = [f for f, v in perm_importance_dict.items() if v > 0.001]\n",
    "else:\n",
    "    perm_positive = all_eng_features\n",
    "\n",
    "recommended = sorted(set(stable_features) & set(perm_positive))\n",
    "\n",
    "if len(recommended) < 5 and consensus_sorted:\n",
    "    recommended = [f for f, _ in consensus_sorted if f in all_eng_features][:20]\n",
    "    print(f\"Fallback: using top {len(recommended)} features by consensus.\")\n",
    "\n",
    "leak_set = {s[\"feature\"] for s in leakage_suspects}\n",
    "recommended = [f for f in recommended if f not in leak_set]\n",
    "\n",
    "report[\"recommended_features\"] = recommended\n",
    "print(f\"\\nRecommended features ({len(recommended)}):\")\n",
    "for f in recommended:\n",
    "    print(f\"  - {f}\")\n",
    "\n",
    "print(f\"\\nEngineered features created ({len(report['engineered_features_created'])}):\")\n",
    "for f in report[\"engineered_features_created\"][:20]:\n",
    "    print(f\"  + {f}\")\n",
    "if len(report[\"engineered_features_created\"]) > 20:\n",
    "    print(f\"  ... and {len(report['engineered_features_created']) - 20} more\")\n",
    "\n",
    "print(f\"\\nDropped features ({len(report['dropped_features'])}):\")\n",
    "for d in report[\"dropped_features\"]:\n",
    "    print(f\"  x {d['feature']}: {d['reason']}\")\n",
    "\n",
    "print(f\"\\nHigh-risk features ({len(report['high_risk_features'])}):\")\n",
    "for h in report[\"high_risk_features\"]:\n",
    "    if isinstance(h, dict):\n",
    "        print(f\"  ! {h.get('feature', h)}: {h.get('reason', '')}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# I \u2014 Export transformed features parquet (Polars)\n",
    "# ---------------------------------------------------------------------------\n",
    "if output_features_parquet_path:\n",
    "    export_cols = [target_col] + recommended\n",
    "    for c in id_cols:\n",
    "        if c in df_eng.columns and c not in export_cols:\n",
    "            export_cols.insert(0, c)\n",
    "    if time_col and time_col in df_eng.columns and time_col not in export_cols:\n",
    "        export_cols.insert(0, time_col)\n",
    "    export_cols = [c for c in export_cols if c in df_eng.columns]\n",
    "    df_export = df_eng.select(export_cols)\n",
    "    Path(output_features_parquet_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "    df_export.write_parquet(output_features_parquet_path)\n",
    "    print(f\"\\nExported {df_export.shape} to {output_features_parquet_path}\")\n",
    "else:\n",
    "    print(\"\\nNo output_features_parquet_path \u2014 skipping parquet export.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# I \u2014 Save metrics JSON\n",
    "# ---------------------------------------------------------------------------\n",
    "report[\"run_metadata\"][\"data_shape\"] = list(df_raw.shape)\n",
    "report[\"run_metadata\"][\"n_features_original\"] = len(feature_candidates) + len(high_null_cols)\n",
    "report[\"run_metadata\"][\"n_features_engineered\"] = len(report[\"engineered_features_created\"])\n",
    "report[\"run_metadata\"][\"n_features_recommended\"] = len(recommended)\n",
    "report[\"run_metadata\"][\"parameters\"] = {\n",
    "    \"input_parquet_paths\": input_parquet_paths,\n",
    "    \"target_col\": target_col,\n",
    "    \"task_type\": task_type,\n",
    "    \"test_size\": test_size,\n",
    "    \"cv_folds\": cv_folds,\n",
    "    \"missingness_drop_threshold\": missingness_drop_threshold,\n",
    "    \"high_cardinality_threshold\": high_cardinality_threshold,\n",
    "    \"rare_category_min_count\": rare_category_min_count,\n",
    "    \"enable_interactions\": enable_interactions,\n",
    "    \"enable_group_aggregations\": enable_group_aggregations,\n",
    "    \"enable_time_features\": enable_time_features,\n",
    "    \"enable_target_encoding\": enable_target_encoding,\n",
    "    \"enable_mutual_info\": enable_mutual_info,\n",
    "    \"enable_permutation_importance\": enable_permutation_importance,\n",
    "    \"enable_stability_checks\": enable_stability_checks,\n",
    "}\n",
    "\n",
    "def json_safe(obj):\n",
    "    \"\"\"Handle non-JSON-serializable types from sklearn results.\"\"\"\n",
    "    if hasattr(obj, \"item\"):  # numpy scalar\n",
    "        return obj.item()\n",
    "    if hasattr(obj, \"tolist\"):  # numpy array\n",
    "        return obj.tolist()\n",
    "    return str(obj)\n",
    "\n",
    "Path(metrics_json_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "with open(metrics_json_path, \"w\") as f:\n",
    "    json.dump(report, f, indent=2, default=json_safe)\n",
    "print(f\"Metrics JSON saved to {metrics_json_path}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "This notebook completed a full feature engineering and selection pipeline:\n",
    "\n",
    "1. **Data audit** \u2014 missingness, cardinality, distributions, outliers\n",
    "2. **Univariate scoring** \u2014 correlation, ANOVA, mutual info, AUC, chi-squared\n",
    "3. **Consensus ranking** \u2014 cross-method agreement\n",
    "4. **Feature engineering** \u2014 missingness indicators, log transforms, interactions,\n",
    "   group aggregations, time features, frequency encoding, optional target encoding, text features\n",
    "5. **Model-based selection** \u2014 L1 regularization, permutation importance, stability selection\n",
    "6. **Leakage & drift checks** \u2014 temporal drift, suspicious features\n",
    "7. **Final recommendations** \u2014 curated feature list with explanations"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# Final summary\n",
    "# ---------------------------------------------------------------------------\n",
    "print(\"=\" * 60)\n",
    "print(\"FEATURE ENGINEERING & SELECTION \u2014 COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"  Task type:              {task_type}\")\n",
    "print(f\"  Original features:      {len(feature_candidates) + len(high_null_cols)}\")\n",
    "print(f\"  Engineered features:    {len(report['engineered_features_created'])}\")\n",
    "print(f\"  Recommended features:   {len(recommended)}\")\n",
    "print(f\"  Dropped features:       {len(report['dropped_features'])}\")\n",
    "print(f\"  High-risk features:     {len(report['high_risk_features'])}\")\n",
    "print(f\"  Metrics JSON:           {metrics_json_path}\")\n",
    "if output_features_parquet_path:\n",
    "    print(f\"  Features parquet:       {output_features_parquet_path}\")\n",
    "print(f\"  Plots directory:        {plots_dir}\")\n",
    "print(\"=\" * 60)"
   ],
   "outputs": [],
   "execution_count": null
  }
 ]
}