{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# 001 — CatBoost Regression\n",
    "\n",
    "Regression with CatBoost, Optuna hyperparameter tuning, residual analysis,\n",
    "SHAP explanations, and structured metrics output.\n",
    "\n",
    "**Lifecycle stage:** seedling (model-garden)\n",
    "\n",
    "All code is self-contained in this notebook — no external library imports\n",
    "from a shared `src/` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# Papermill parameters  (this cell is tagged \"parameters\")\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "# Data loading\n",
    "feature_paths: list[str] = []          # local or gs:// URIs; empty → synthetic\n",
    "target_paths: list[str] = []           # optional separate target files\n",
    "join_key: str | None = None            # key to join features ↔ targets\n",
    "feature_cols: list[str] | None = None  # subset of columns; None → all\n",
    "target_col: str = \"target\"\n",
    "entity_id: str | None = \"entity_id\"    # column for group-based train/test split\n",
    "\n",
    "# Splitting\n",
    "test_size: float = 0.2\n",
    "random_state: int = 42\n",
    "\n",
    "# Optuna\n",
    "optuna_n_trials: int = 30\n",
    "optuna_timeout_s: int | None = None\n",
    "optimize_metric: str = \"rmse\"  # \"rmse\", \"mae\", \"r2\", \"mape\"\n",
    "\n",
    "# Outputs\n",
    "metrics_json_path: str = \"outputs/metrics/metrics.json\"\n",
    "model_output_path: str = \"outputs/models/model.cbm\"\n",
    "executed_notebook_path: str | None = None\n",
    "plots_dir: str = \"outputs/plots\"\n",
    "\n",
    "# SHAP / feature importance\n",
    "shap_sample_size: int = 1000\n",
    "enable_shap: bool = True\n",
    "enable_feature_importance: bool = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# Imports\n",
    "# ---------------------------------------------------------------------------\n",
    "import json\n",
    "import os\n",
    "import warnings\n",
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "\n",
    "# Suppress all warnings (keeps notebook output and rendered HTML clean)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ[\"PYTHONWARNINGS\"] = \"ignore\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "from scipy import stats\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.metrics import (\n",
    "    mean_absolute_error,\n",
    "    mean_absolute_percentage_error,\n",
    "    mean_squared_error,\n",
    "    median_absolute_error,\n",
    "    max_error,\n",
    "    r2_score,\n",
    ")\n",
    "from sklearn.model_selection import GroupShuffleSplit, KFold, train_test_split\n",
    "\n",
    "# Ensure output dirs exist\n",
    "for d in [\"outputs/runs\", \"outputs/plots\", \"outputs/models\", \"outputs/metrics\"]:\n",
    "    Path(d).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Run started at {datetime.now(timezone.utc).isoformat()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## 1 — Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# Data loading helpers (all inline, no external modules)\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def _read_file(path: str) -> pl.DataFrame:\n",
    "    \"\"\"Read a single parquet or csv file (local or gs://).\"\"\"\n",
    "    p = path.strip()\n",
    "    if p.endswith(\".parquet\") or p.endswith(\".pq\"):\n",
    "        return pl.read_parquet(p)\n",
    "    return pl.read_csv(p)\n",
    "\n",
    "\n",
    "def load_data(\n",
    "    feature_paths: list[str],\n",
    "    target_paths: list[str],\n",
    "    join_key: str | None,\n",
    "    feature_cols: list[str] | None,\n",
    "    target_col: str,\n",
    "    entity_id: str | None,\n",
    ") -> pl.DataFrame:\n",
    "    \"\"\"Load features (and optional targets), returning a single DataFrame.\"\"\"\n",
    "\n",
    "    if not feature_paths:\n",
    "        # Generate synthetic regression dataset with entity_id\n",
    "        print(\"No feature_paths provided — generating synthetic dataset.\")\n",
    "        n_samples = 5_000\n",
    "        X, y = make_regression(\n",
    "            n_samples=n_samples,\n",
    "            n_features=20,\n",
    "            n_informative=12,\n",
    "            noise=15.0,\n",
    "            random_state=random_state,\n",
    "        )\n",
    "        cols = {f\"feat_{i:02d}\": X[:, i] for i in range(X.shape[1])}\n",
    "        cols[target_col] = y\n",
    "        # Assign ~3 rows per entity on average so group split is meaningful\n",
    "        if entity_id:\n",
    "            rng = np.random.RandomState(random_state)\n",
    "            n_entities = n_samples // 3\n",
    "            cols[entity_id] = rng.randint(0, n_entities, size=n_samples)\n",
    "        return pl.DataFrame(cols)\n",
    "\n",
    "    # Read and concat feature files\n",
    "    dfs = [_read_file(p) for p in feature_paths]\n",
    "    df = pl.concat(dfs, how=\"vertical_relaxed\")\n",
    "\n",
    "    # Optionally join separate target files\n",
    "    if target_paths:\n",
    "        tgt_dfs = [_read_file(p) for p in target_paths]\n",
    "        tgt = pl.concat(tgt_dfs, how=\"vertical_relaxed\")\n",
    "        if join_key:\n",
    "            df = df.join(tgt, on=join_key, how=\"inner\")\n",
    "        else:\n",
    "            df = pl.concat([df, tgt], how=\"horizontal\")\n",
    "\n",
    "    # Subset columns if requested\n",
    "    if feature_cols is not None:\n",
    "        keep = list(set(feature_cols + [target_col] + ([entity_id] if entity_id else [])))\n",
    "        df = df.select([c for c in keep if c in df.columns])\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "df = load_data(feature_paths, target_paths, join_key, feature_cols, target_col, entity_id)\n",
    "print(f\"Loaded DataFrame: {df.shape[0]:,} rows × {df.shape[1]} cols\")\n",
    "if entity_id and entity_id in df.columns:\n",
    "    print(f\"Unique entities ({entity_id}): {df[entity_id].n_unique():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## 2 — EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# Schema & nulls\n",
    "# ---------------------------------------------------------------------------\n",
    "print(\"Schema:\")\n",
    "for name, dtype in zip(df.columns, df.dtypes):\n",
    "    null_ct = df[name].null_count()\n",
    "    print(f\"  {name:30s}  {str(dtype):15s}  nulls={null_ct}\")\n",
    "\n",
    "print(f\"\\nTarget summary ({target_col}):\")\n",
    "target_series = df[target_col]\n",
    "print(f\"  count:  {target_series.len():,}\")\n",
    "print(f\"  mean:   {target_series.mean():.4f}\")\n",
    "print(f\"  std:    {target_series.std():.4f}\")\n",
    "print(f\"  min:    {target_series.min():.4f}\")\n",
    "print(f\"  25%:    {target_series.quantile(0.25):.4f}\")\n",
    "print(f\"  50%:    {target_series.quantile(0.50):.4f}\")\n",
    "print(f\"  75%:    {target_series.quantile(0.75):.4f}\")\n",
    "print(f\"  max:    {target_series.max():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# Summary stats for numeric columns\n",
    "# ---------------------------------------------------------------------------\n",
    "numeric_cols = [c for c in df.columns if df[c].dtype in (pl.Float32, pl.Float64, pl.Int8, pl.Int16, pl.Int32, pl.Int64, pl.UInt8, pl.UInt16, pl.UInt32, pl.UInt64)]\n",
    "feature_numeric = [c for c in numeric_cols if c != target_col]\n",
    "\n",
    "if feature_numeric:\n",
    "    print(df.select(feature_numeric).describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# Target distribution\n",
    "# ---------------------------------------------------------------------------\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Histogram\n",
    "ax = axes[0]\n",
    "target_vals = df[target_col].drop_nulls().to_numpy()\n",
    "ax.hist(target_vals, bins=50, edgecolor=\"white\", color=\"#2196F3\")\n",
    "ax.axvline(np.mean(target_vals), color=\"red\", linestyle=\"--\", alpha=0.7, label=f\"Mean={np.mean(target_vals):.2f}\")\n",
    "ax.axvline(np.median(target_vals), color=\"orange\", linestyle=\"--\", alpha=0.7, label=f\"Median={np.median(target_vals):.2f}\")\n",
    "ax.set_xlabel(target_col)\n",
    "ax.set_ylabel(\"Count\")\n",
    "ax.set_title(f\"Target Distribution: {target_col}\")\n",
    "ax.legend(fontsize=8)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Box plot\n",
    "ax = axes[1]\n",
    "bp = ax.boxplot(target_vals, vert=True, patch_artist=True)\n",
    "bp[\"boxes\"][0].set_facecolor(\"#2196F3\")\n",
    "bp[\"boxes\"][0].set_alpha(0.6)\n",
    "ax.set_ylabel(target_col)\n",
    "ax.set_title(f\"Target Box Plot: {target_col}\")\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(f\"{plots_dir}/eda_target_distribution.png\", dpi=120)\n",
    "plt.show()\n",
    "print(f\"Saved → {plots_dir}/eda_target_distribution.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# Histograms (first 8 numeric features)\n",
    "# ---------------------------------------------------------------------------\n",
    "plot_cols = feature_numeric[:8]\n",
    "if plot_cols:\n",
    "    n = len(plot_cols)\n",
    "    ncols = min(4, n)\n",
    "    nrows = (n + ncols - 1) // ncols\n",
    "    fig, axes = plt.subplots(nrows, ncols, figsize=(4 * ncols, 3 * nrows))\n",
    "    axes_flat = np.array(axes).flatten() if n > 1 else [axes]\n",
    "    for i, col in enumerate(plot_cols):\n",
    "        axes_flat[i].hist(df[col].drop_nulls().to_numpy(), bins=40, edgecolor=\"white\")\n",
    "        axes_flat[i].set_title(col, fontsize=10)\n",
    "    for j in range(i + 1, len(axes_flat)):\n",
    "        axes_flat[j].set_visible(False)\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(f\"{plots_dir}/eda_histograms.png\", dpi=120)\n",
    "    plt.show()\n",
    "    print(f\"Saved → {plots_dir}/eda_histograms.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# Correlation heatmap (cap at 20 features to keep runtime sane)\n",
    "# ---------------------------------------------------------------------------\n",
    "corr_cols = feature_numeric[:20]\n",
    "if len(corr_cols) >= 2:\n",
    "    corr_df = df.select(corr_cols).to_pandas()\n",
    "    corr = corr_df.corr()\n",
    "    fig, ax = plt.subplots(figsize=(max(6, len(corr_cols) * 0.6), max(5, len(corr_cols) * 0.5)))\n",
    "    im = ax.imshow(corr.values, aspect=\"auto\", cmap=\"RdBu_r\", vmin=-1, vmax=1)\n",
    "    ax.set_xticks(range(len(corr_cols)))\n",
    "    ax.set_yticks(range(len(corr_cols)))\n",
    "    ax.set_xticklabels(corr_cols, rotation=90, fontsize=7)\n",
    "    ax.set_yticklabels(corr_cols, fontsize=7)\n",
    "    fig.colorbar(im, ax=ax, shrink=0.8)\n",
    "    ax.set_title(\"Pairwise Correlation\")\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(f\"{plots_dir}/eda_correlation.png\", dpi=120)\n",
    "    plt.show()\n",
    "    print(f\"Saved → {plots_dir}/eda_correlation.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# Feature vs Target scatter plots (top 6 by absolute correlation)\n",
    "# ---------------------------------------------------------------------------\n",
    "if feature_numeric:\n",
    "    # Compute correlation of each feature with target\n",
    "    target_np = df[target_col].drop_nulls().to_numpy()\n",
    "    feat_corrs = []\n",
    "    for col in feature_numeric:\n",
    "        vals = df[col].drop_nulls().to_numpy()\n",
    "        if len(vals) == len(target_np) and len(vals) > 1:\n",
    "            corr_val = np.corrcoef(vals, target_np)[0, 1]\n",
    "            feat_corrs.append((col, abs(corr_val), corr_val))\n",
    "    feat_corrs.sort(key=lambda x: x[1], reverse=True)\n",
    "    top_corr_feats = feat_corrs[:6]\n",
    "\n",
    "    if top_corr_feats:\n",
    "        n = len(top_corr_feats)\n",
    "        ncols = min(3, n)\n",
    "        nrows = (n + ncols - 1) // ncols\n",
    "        fig, axes = plt.subplots(nrows, ncols, figsize=(5 * ncols, 4 * nrows))\n",
    "        axes_flat = np.array(axes).flatten() if n > 1 else [axes]\n",
    "\n",
    "        for i, (col, abs_corr, corr_val) in enumerate(top_corr_feats):\n",
    "            ax = axes_flat[i]\n",
    "            x_vals = df[col].to_numpy()\n",
    "            ax.scatter(x_vals, target_np, alpha=0.15, s=8, color=\"#2196F3\")\n",
    "            # Trend line\n",
    "            z = np.polyfit(x_vals, target_np, 1)\n",
    "            p = np.poly1d(z)\n",
    "            x_sorted = np.sort(x_vals)\n",
    "            ax.plot(x_sorted, p(x_sorted), color=\"red\", linewidth=2, alpha=0.7)\n",
    "            ax.set_xlabel(col)\n",
    "            ax.set_ylabel(target_col)\n",
    "            ax.set_title(f\"{col} (r={corr_val:.3f})\")\n",
    "            ax.grid(True, alpha=0.3)\n",
    "\n",
    "        for j in range(i + 1, len(axes_flat)):\n",
    "            axes_flat[j].set_visible(False)\n",
    "\n",
    "        fig.suptitle(\"Top Features vs Target (by correlation)\", fontsize=13, y=1.02)\n",
    "        fig.tight_layout()\n",
    "        fig.savefig(f\"{plots_dir}/eda_feature_vs_target.png\", dpi=120, bbox_inches=\"tight\")\n",
    "        plt.show()\n",
    "        print(f\"Saved → {plots_dir}/eda_feature_vs_target.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## 3 — Train / Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# Split — group-based on entity_id (no entity leaks between train/test)\n",
    "# ---------------------------------------------------------------------------\n",
    "non_feature_cols = {target_col}\n",
    "if entity_id and entity_id in df.columns:\n",
    "    non_feature_cols.add(entity_id)\n",
    "\n",
    "feature_names = [c for c in df.columns if c not in non_feature_cols]\n",
    "X_all = df.select(feature_names).to_numpy()\n",
    "y_all = df[target_col].to_numpy().astype(float)\n",
    "\n",
    "if entity_id and entity_id in df.columns:\n",
    "    groups = df[entity_id].to_numpy()\n",
    "    gss = GroupShuffleSplit(n_splits=1, test_size=test_size, random_state=random_state)\n",
    "    train_idx, test_idx = next(gss.split(X_all, y_all, groups=groups))\n",
    "    X_train, X_test = X_all[train_idx], X_all[test_idx]\n",
    "    y_train, y_test = y_all[train_idx], y_all[test_idx]\n",
    "    n_train_entities = len(set(groups[train_idx]))\n",
    "    n_test_entities = len(set(groups[test_idx]))\n",
    "    assert len(set(groups[train_idx]) & set(groups[test_idx])) == 0, \"Entity leak detected!\"\n",
    "    print(f\"Group split on '{entity_id}': {n_train_entities:,} train entities, {n_test_entities:,} test entities\")\n",
    "else:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_all, y_all, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "    print(\"Random split (no entity_id column found)\")\n",
    "\n",
    "print(f\"Train: {X_train.shape[0]:,}  |  Test: {X_test.shape[0]:,}\")\n",
    "print(f\"Train target — mean: {y_train.mean():.4f}  std: {y_train.std():.4f}\")\n",
    "print(f\"Test target  — mean: {y_test.mean():.4f}  std: {y_test.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## 4 — Optuna Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# Optuna objective\n",
    "# ---------------------------------------------------------------------------\n",
    "import optuna\n",
    "\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "\n",
    "def _score_regression(y_true, y_pred, metric: str) -> float:\n",
    "    \"\"\"Return a score for a regression metric (higher is always better).\"\"\"\n",
    "    if metric == \"rmse\":\n",
    "        return -np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    elif metric == \"mae\":\n",
    "        return -mean_absolute_error(y_true, y_pred)\n",
    "    elif metric == \"r2\":\n",
    "        return r2_score(y_true, y_pred)\n",
    "    elif metric == \"mape\":\n",
    "        return -mean_absolute_percentage_error(y_true, y_pred)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown metric: {metric}\")\n",
    "\n",
    "\n",
    "def objective(trial: optuna.Trial) -> float:\n",
    "    params = {\n",
    "        \"iterations\": trial.suggest_int(\"iterations\", 200, 1500),\n",
    "        \"depth\": trial.suggest_int(\"depth\", 3, 10),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True),\n",
    "        \"l2_leaf_reg\": trial.suggest_float(\"l2_leaf_reg\", 1e-2, 10.0, log=True),\n",
    "        \"border_count\": trial.suggest_int(\"border_count\", 32, 255),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "        \"random_strength\": trial.suggest_float(\"random_strength\", 1e-3, 10.0, log=True),\n",
    "    }\n",
    "\n",
    "    kf = KFold(n_splits=3, shuffle=True, random_state=random_state)\n",
    "    fold_scores = []\n",
    "\n",
    "    for train_idx, val_idx in kf.split(X_train):\n",
    "        xtr, xvl = X_train[train_idx], X_train[val_idx]\n",
    "        ytr, yvl = y_train[train_idx], y_train[val_idx]\n",
    "\n",
    "        model = CatBoostRegressor(\n",
    "            **params,\n",
    "            eval_metric=\"RMSE\",\n",
    "            random_seed=random_state,\n",
    "            verbose=0,\n",
    "            early_stopping_rounds=50,\n",
    "        )\n",
    "        model.fit(xtr, ytr, eval_set=(xvl, yvl), verbose=0)\n",
    "\n",
    "        preds = model.predict(xvl)\n",
    "        score = _score_regression(yvl, preds, optimize_metric)\n",
    "        fold_scores.append(score)\n",
    "\n",
    "    return float(np.mean(fold_scores))\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=optuna_n_trials, timeout=optuna_timeout_s)\n",
    "\n",
    "best_params = study.best_params\n",
    "print(f\"Best CV {optimize_metric}: {study.best_value:.4f}\")\n",
    "print(f\"Best params: {json.dumps(best_params, indent=2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## 5 — Train Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# Final model on full train set\n",
    "# ---------------------------------------------------------------------------\n",
    "final_model = CatBoostRegressor(\n",
    "    **best_params,\n",
    "    eval_metric=\"RMSE\",\n",
    "    random_seed=random_state,\n",
    "    verbose=100,\n",
    ")\n",
    "final_model.fit(X_train, y_train)\n",
    "\n",
    "# Save model\n",
    "Path(model_output_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "final_model.save_model(model_output_path)\n",
    "print(f\"\\nModel saved → {model_output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## 6 — Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# Compute predictions and metrics on test set\n",
    "# ---------------------------------------------------------------------------\n",
    "y_pred = final_model.predict(X_test)\n",
    "residuals = y_test - y_pred\n",
    "\n",
    "# Core metrics\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "medae = median_absolute_error(y_test, y_pred)\n",
    "max_err = max_error(y_test, y_pred)\n",
    "\n",
    "# Additional stats\n",
    "explained_variance = 1 - np.var(residuals) / np.var(y_test)\n",
    "mean_residual = np.mean(residuals)\n",
    "std_residual = np.std(residuals)\n",
    "\n",
    "print(\"Test Set Metrics\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"  RMSE:                {rmse:,.4f}\")\n",
    "print(f\"  MAE:                 {mae:,.4f}\")\n",
    "print(f\"  R²:                  {r2:.4f}\")\n",
    "print(f\"  MAPE:                {mape:.4f}\")\n",
    "print(f\"  Median AE:           {medae:,.4f}\")\n",
    "print(f\"  Max Error:           {max_err:,.4f}\")\n",
    "print(f\"  Explained Variance:  {explained_variance:.4f}\")\n",
    "print(f\"  Mean Residual:       {mean_residual:,.4f}\")\n",
    "print(f\"  Std Residual:        {std_residual:,.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# Actual vs Predicted scatter plot\n",
    "# ---------------------------------------------------------------------------\n",
    "fig, ax = plt.subplots(figsize=(7, 7))\n",
    "ax.scatter(y_test, y_pred, alpha=0.2, s=12, color=\"#2196F3\", edgecolor=\"none\")\n",
    "\n",
    "# Perfect prediction line\n",
    "lims = [\n",
    "    min(y_test.min(), y_pred.min()),\n",
    "    max(y_test.max(), y_pred.max()),\n",
    "]\n",
    "margin = (lims[1] - lims[0]) * 0.05\n",
    "lims = [lims[0] - margin, lims[1] + margin]\n",
    "ax.plot(lims, lims, \"k--\", alpha=0.5, linewidth=1.5, label=\"Perfect prediction\")\n",
    "\n",
    "# Best fit line\n",
    "z = np.polyfit(y_test, y_pred, 1)\n",
    "p = np.poly1d(z)\n",
    "x_sorted = np.sort(y_test)\n",
    "ax.plot(x_sorted, p(x_sorted), color=\"red\", linewidth=2, alpha=0.7, label=f\"Fit: y={z[0]:.3f}x+{z[1]:.1f}\")\n",
    "\n",
    "ax.set_xlabel(\"Actual\")\n",
    "ax.set_ylabel(\"Predicted\")\n",
    "ax.set_title(f\"Actual vs Predicted (R²={r2:.4f})\")\n",
    "ax.legend(loc=\"upper left\")\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xlim(lims)\n",
    "ax.set_ylim(lims)\n",
    "ax.set_aspect(\"equal\")\n",
    "fig.tight_layout()\n",
    "fig.savefig(f\"{plots_dir}/actual_vs_predicted.png\", dpi=120)\n",
    "plt.show()\n",
    "print(f\"Saved → {plots_dir}/actual_vs_predicted.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# Residuals vs Predicted\n",
    "# ---------------------------------------------------------------------------\n",
    "fig, ax = plt.subplots(figsize=(9, 5))\n",
    "ax.scatter(y_pred, residuals, alpha=0.2, s=12, color=\"#4CAF50\", edgecolor=\"none\")\n",
    "ax.axhline(0, color=\"red\", linestyle=\"--\", linewidth=1.5, alpha=0.7)\n",
    "\n",
    "# Add ±1 std bands\n",
    "ax.axhline(std_residual, color=\"orange\", linestyle=\":\", alpha=0.5, label=f\"+1 std ({std_residual:,.2f})\")\n",
    "ax.axhline(-std_residual, color=\"orange\", linestyle=\":\", alpha=0.5, label=f\"-1 std ({-std_residual:,.2f})\")\n",
    "\n",
    "# LOWESS-style smoothed trend (binned means)\n",
    "n_bins = 30\n",
    "pred_sorted_idx = np.argsort(y_pred)\n",
    "bin_size = len(y_pred) // n_bins\n",
    "if bin_size > 0:\n",
    "    bin_centers = []\n",
    "    bin_means = []\n",
    "    for b in range(n_bins):\n",
    "        start = b * bin_size\n",
    "        end = start + bin_size if b < n_bins - 1 else len(y_pred)\n",
    "        idx = pred_sorted_idx[start:end]\n",
    "        bin_centers.append(np.mean(y_pred[idx]))\n",
    "        bin_means.append(np.mean(residuals[idx]))\n",
    "    ax.plot(bin_centers, bin_means, color=\"darkblue\", linewidth=2, alpha=0.8, label=\"Binned mean\")\n",
    "\n",
    "ax.set_xlabel(\"Predicted\")\n",
    "ax.set_ylabel(\"Residual (Actual - Predicted)\")\n",
    "ax.set_title(\"Residuals vs Predicted\")\n",
    "ax.legend(fontsize=8)\n",
    "ax.grid(True, alpha=0.3)\n",
    "fig.tight_layout()\n",
    "fig.savefig(f\"{plots_dir}/residuals_vs_predicted.png\", dpi=120)\n",
    "plt.show()\n",
    "print(f\"Saved → {plots_dir}/residuals_vs_predicted.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# Residual Distribution\n",
    "# ---------------------------------------------------------------------------\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Histogram with normal fit\n",
    "ax = axes[0]\n",
    "ax.hist(residuals, bins=50, density=True, edgecolor=\"white\", color=\"#2196F3\", alpha=0.7, label=\"Residuals\")\n",
    "# Overlay normal distribution\n",
    "x_range = np.linspace(residuals.min(), residuals.max(), 200)\n",
    "ax.plot(x_range, stats.norm.pdf(x_range, mean_residual, std_residual),\n",
    "        color=\"red\", linewidth=2, label=f\"Normal(μ={mean_residual:.2f}, σ={std_residual:.2f})\")\n",
    "ax.set_xlabel(\"Residual\")\n",
    "ax.set_ylabel(\"Density\")\n",
    "ax.set_title(\"Residual Distribution\")\n",
    "ax.legend(fontsize=8)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Q-Q Plot\n",
    "ax = axes[1]\n",
    "standardized_residuals = (residuals - mean_residual) / std_residual\n",
    "theoretical_quantiles = stats.norm.ppf(np.linspace(0.001, 0.999, len(standardized_residuals)))\n",
    "sorted_residuals = np.sort(standardized_residuals)\n",
    "ax.scatter(theoretical_quantiles, sorted_residuals, alpha=0.3, s=8, color=\"#4CAF50\")\n",
    "qq_lims = [min(theoretical_quantiles.min(), sorted_residuals.min()),\n",
    "           max(theoretical_quantiles.max(), sorted_residuals.max())]\n",
    "ax.plot(qq_lims, qq_lims, \"r--\", linewidth=1.5, alpha=0.7, label=\"Normal reference\")\n",
    "ax.set_xlabel(\"Theoretical Quantiles\")\n",
    "ax.set_ylabel(\"Standardized Residuals\")\n",
    "ax.set_title(\"Q-Q Plot (Residuals vs Normal)\")\n",
    "ax.legend(fontsize=8)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(f\"{plots_dir}/residual_distribution.png\", dpi=120)\n",
    "plt.show()\n",
    "print(f\"Saved → {plots_dir}/residual_distribution.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# Prediction Error Plot (sorted index)\n",
    "# ---------------------------------------------------------------------------\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "sort_idx = np.argsort(y_test)\n",
    "y_test_sorted = y_test[sort_idx]\n",
    "y_pred_sorted = y_pred[sort_idx]\n",
    "x_axis = np.arange(len(y_test_sorted))\n",
    "\n",
    "ax.plot(x_axis, y_test_sorted, color=\"#2196F3\", linewidth=1, alpha=0.8, label=\"Actual\")\n",
    "ax.plot(x_axis, y_pred_sorted, color=\"#FF5722\", linewidth=1, alpha=0.6, label=\"Predicted\")\n",
    "ax.fill_between(x_axis, y_test_sorted, y_pred_sorted, alpha=0.15, color=\"red\", label=\"Error\")\n",
    "\n",
    "ax.set_xlabel(\"Sample (sorted by actual value)\")\n",
    "ax.set_ylabel(\"Value\")\n",
    "ax.set_title(\"Prediction Error: Actual vs Predicted (sorted)\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "fig.tight_layout()\n",
    "fig.savefig(f\"{plots_dir}/prediction_error_sorted.png\", dpi=120)\n",
    "plt.show()\n",
    "print(f\"Saved → {plots_dir}/prediction_error_sorted.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# Cumulative Error Distribution\n",
    "# ---------------------------------------------------------------------------\n",
    "abs_errors = np.abs(residuals)\n",
    "sorted_errors = np.sort(abs_errors)\n",
    "cumulative_pct = np.arange(1, len(sorted_errors) + 1) / len(sorted_errors) * 100\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9, 5))\n",
    "ax.plot(sorted_errors, cumulative_pct, linewidth=2, color=\"#9C27B0\")\n",
    "\n",
    "# Mark key percentiles\n",
    "for pct in [50, 75, 90, 95]:\n",
    "    idx = int(len(sorted_errors) * pct / 100) - 1\n",
    "    err_at_pct = sorted_errors[idx]\n",
    "    ax.axhline(pct, color=\"grey\", linestyle=\":\", alpha=0.3)\n",
    "    ax.axvline(err_at_pct, color=\"grey\", linestyle=\":\", alpha=0.3)\n",
    "    ax.plot(err_at_pct, pct, \"ro\", markersize=6)\n",
    "    ax.annotate(f\"  {pct}%: err≤{err_at_pct:,.2f}\",\n",
    "                xy=(err_at_pct, pct), fontsize=8, va=\"bottom\")\n",
    "\n",
    "ax.set_xlabel(\"Absolute Error\")\n",
    "ax.set_ylabel(\"Cumulative % of Predictions\")\n",
    "ax.set_title(\"Cumulative Error Distribution\")\n",
    "ax.grid(True, alpha=0.3)\n",
    "fig.tight_layout()\n",
    "fig.savefig(f\"{plots_dir}/cumulative_error_distribution.png\", dpi=120)\n",
    "plt.show()\n",
    "print(f\"Saved → {plots_dir}/cumulative_error_distribution.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# Error by Decile of Actual Value\n",
    "# ---------------------------------------------------------------------------\n",
    "n_deciles = 10\n",
    "decile_edges = np.percentile(y_test, np.linspace(0, 100, n_deciles + 1))\n",
    "decile_labels = []\n",
    "decile_mae = []\n",
    "decile_rmse = []\n",
    "decile_counts = []\n",
    "\n",
    "for i in range(n_deciles):\n",
    "    lo, hi = decile_edges[i], decile_edges[i + 1]\n",
    "    if i < n_deciles - 1:\n",
    "        mask = (y_test >= lo) & (y_test < hi)\n",
    "    else:\n",
    "        mask = (y_test >= lo) & (y_test <= hi)\n",
    "    if mask.sum() == 0:\n",
    "        continue\n",
    "    decile_labels.append(f\"D{i+1}\\n[{lo:,.0f},{hi:,.0f})\")\n",
    "    decile_mae.append(mean_absolute_error(y_test[mask], y_pred[mask]))\n",
    "    decile_rmse.append(np.sqrt(mean_squared_error(y_test[mask], y_pred[mask])))\n",
    "    decile_counts.append(int(mask.sum()))\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "x = np.arange(len(decile_labels))\n",
    "width = 0.35\n",
    "\n",
    "# MAE & RMSE by decile\n",
    "ax = axes[0]\n",
    "bars1 = ax.bar(x - width/2, decile_mae, width, label=\"MAE\", color=\"#2196F3\", alpha=0.8)\n",
    "bars2 = ax.bar(x + width/2, decile_rmse, width, label=\"RMSE\", color=\"#FF9800\", alpha=0.8)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(decile_labels, fontsize=7)\n",
    "ax.set_ylabel(\"Error\")\n",
    "ax.set_title(\"MAE & RMSE by Decile of Actual Value\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis=\"y\")\n",
    "\n",
    "# Sample counts by decile\n",
    "ax = axes[1]\n",
    "ax.bar(x, decile_counts, color=\"#4CAF50\", alpha=0.8)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(decile_labels, fontsize=7)\n",
    "ax.set_ylabel(\"Count\")\n",
    "ax.set_title(\"Sample Count by Decile\")\n",
    "ax.grid(True, alpha=0.3, axis=\"y\")\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(f\"{plots_dir}/error_by_decile.png\", dpi=120)\n",
    "plt.show()\n",
    "print(f\"Saved → {plots_dir}/error_by_decile.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# Residuals vs Top Features\n",
    "# ---------------------------------------------------------------------------\n",
    "importances_quick = final_model.get_feature_importance()\n",
    "imp_order = np.argsort(-importances_quick)\n",
    "top_feat_indices = imp_order[:4]\n",
    "\n",
    "fig, axes = plt.subplots(1, len(top_feat_indices), figsize=(5 * len(top_feat_indices), 4))\n",
    "if len(top_feat_indices) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for ax, fi in zip(axes, top_feat_indices):\n",
    "    feat_vals = X_test[:, fi]\n",
    "    ax.scatter(feat_vals, residuals, alpha=0.15, s=8, color=\"#FF5722\")\n",
    "    ax.axhline(0, color=\"red\", linestyle=\"--\", linewidth=1, alpha=0.5)\n",
    "\n",
    "    # Binned mean trend\n",
    "    n_bins = 20\n",
    "    sorted_idx = np.argsort(feat_vals)\n",
    "    bin_size = len(feat_vals) // n_bins\n",
    "    if bin_size > 0:\n",
    "        bx, by = [], []\n",
    "        for b in range(n_bins):\n",
    "            s = b * bin_size\n",
    "            e = s + bin_size if b < n_bins - 1 else len(feat_vals)\n",
    "            idx = sorted_idx[s:e]\n",
    "            bx.append(np.mean(feat_vals[idx]))\n",
    "            by.append(np.mean(residuals[idx]))\n",
    "        ax.plot(bx, by, color=\"darkblue\", linewidth=2, alpha=0.8)\n",
    "\n",
    "    ax.set_xlabel(feature_names[fi])\n",
    "    ax.set_ylabel(\"Residual\")\n",
    "    ax.set_title(f\"Residual vs {feature_names[fi]}\")\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(f\"{plots_dir}/residuals_vs_features.png\", dpi=120)\n",
    "plt.show()\n",
    "print(f\"Saved → {plots_dir}/residuals_vs_features.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# Scale-Location Plot (√|standardized residuals| vs predicted)\n",
    "# ---------------------------------------------------------------------------\n",
    "fig, ax = plt.subplots(figsize=(9, 5))\n",
    "\n",
    "sqrt_abs_std_resid = np.sqrt(np.abs(standardized_residuals))\n",
    "ax.scatter(y_pred, sqrt_abs_std_resid, alpha=0.2, s=12, color=\"#FF9800\", edgecolor=\"none\")\n",
    "\n",
    "# Binned mean trend\n",
    "n_bins = 30\n",
    "pred_sorted_idx = np.argsort(y_pred)\n",
    "bin_size = len(y_pred) // n_bins\n",
    "if bin_size > 0:\n",
    "    bx, by = [], []\n",
    "    for b in range(n_bins):\n",
    "        s = b * bin_size\n",
    "        e = s + bin_size if b < n_bins - 1 else len(y_pred)\n",
    "        idx = pred_sorted_idx[s:e]\n",
    "        bx.append(np.mean(y_pred[idx]))\n",
    "        by.append(np.mean(sqrt_abs_std_resid[idx]))\n",
    "    ax.plot(bx, by, color=\"red\", linewidth=2, alpha=0.8, label=\"Binned mean\")\n",
    "\n",
    "ax.set_xlabel(\"Predicted\")\n",
    "ax.set_ylabel(\"√|Standardized Residual|\")\n",
    "ax.set_title(\"Scale-Location Plot (Homoscedasticity Check)\")\n",
    "ax.legend(fontsize=8)\n",
    "ax.grid(True, alpha=0.3)\n",
    "fig.tight_layout()\n",
    "fig.savefig(f\"{plots_dir}/scale_location.png\", dpi=120)\n",
    "plt.show()\n",
    "print(f\"Saved → {plots_dir}/scale_location.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# Metrics Dashboard — compact summary of all key metrics\n",
    "# ---------------------------------------------------------------------------\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 9))\n",
    "\n",
    "metric_names = [\"RMSE\", \"MAE\", \"R²\", \"MAPE\", \"Median AE\", \"Max Error\"]\n",
    "metric_values = [rmse, mae, r2, mape, medae, max_err]\n",
    "metric_colors = [\"#2196F3\", \"#4CAF50\", \"#FF9800\", \"#9C27B0\", \"#00BCD4\", \"#F44336\"]\n",
    "metric_formats = [\"{:,.4f}\", \"{:,.4f}\", \"{:.4f}\", \"{:.4f}\", \"{:,.4f}\", \"{:,.4f}\"]\n",
    "\n",
    "for ax, name, val, color, fmt in zip(axes.flat, metric_names, metric_values, metric_colors, metric_formats):\n",
    "    ax.barh([name], [val], color=color, alpha=0.8, height=0.5)\n",
    "    ax.text(val, 0, \"  \" + fmt.format(val), va=\"center\", fontsize=16, fontweight=\"bold\")\n",
    "    ax.set_title(name, fontsize=14)\n",
    "    ax.set_xlim(0, val * 1.4 if val > 0 else 1)\n",
    "    ax.grid(True, alpha=0.3, axis=\"x\")\n",
    "    ax.set_yticks([])\n",
    "\n",
    "fig.suptitle(\"Regression Metrics Dashboard\", fontsize=16, y=1.01)\n",
    "fig.tight_layout()\n",
    "fig.savefig(f\"{plots_dir}/metrics_dashboard.png\", dpi=120, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "print(f\"Saved → {plots_dir}/metrics_dashboard.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# Error Percentile Table\n",
    "# ---------------------------------------------------------------------------\n",
    "percentiles = [10, 25, 50, 75, 90, 95, 99]\n",
    "pct_values = np.percentile(abs_errors, percentiles)\n",
    "\n",
    "pct_rows = [{\"percentile\": p, \"abs_error_at_pct\": round(float(v), 4)} for p, v in zip(percentiles, pct_values)]\n",
    "pct_df = pl.DataFrame(pct_rows)\n",
    "print(\"Absolute Error Percentiles:\")\n",
    "print(pct_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-30",
   "metadata": {},
   "source": [
    "## 7 — Metrics JSON Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# Write structured metrics JSON\n",
    "# ---------------------------------------------------------------------------\n",
    "metrics_output = {\n",
    "    \"run_metadata\": {\n",
    "        \"timestamp\": datetime.now(timezone.utc).isoformat(),\n",
    "        \"target_col\": target_col,\n",
    "        \"entity_id\": entity_id,\n",
    "        \"test_size\": test_size,\n",
    "        \"random_state\": random_state,\n",
    "        \"optuna_n_trials\": optuna_n_trials,\n",
    "        \"optimize_metric\": optimize_metric,\n",
    "        \"n_train\": int(X_train.shape[0]),\n",
    "        \"n_test\": int(X_test.shape[0]),\n",
    "        \"n_features\": int(X_train.shape[1]),\n",
    "        \"feature_names\": feature_names,\n",
    "        \"best_optuna_cv_score\": round(study.best_value, 4),\n",
    "        \"best_params\": best_params,\n",
    "        \"model_output_path\": model_output_path,\n",
    "    },\n",
    "    \"test_metrics\": {\n",
    "        \"rmse\": round(rmse, 4),\n",
    "        \"mae\": round(mae, 4),\n",
    "        \"r2\": round(r2, 4),\n",
    "        \"mape\": round(mape, 4),\n",
    "        \"median_ae\": round(medae, 4),\n",
    "        \"max_error\": round(max_err, 4),\n",
    "        \"explained_variance\": round(explained_variance, 4),\n",
    "        \"mse\": round(mse, 4),\n",
    "    },\n",
    "    \"residual_stats\": {\n",
    "        \"mean\": round(float(mean_residual), 4),\n",
    "        \"std\": round(float(std_residual), 4),\n",
    "        \"min\": round(float(residuals.min()), 4),\n",
    "        \"max\": round(float(residuals.max()), 4),\n",
    "        \"skewness\": round(float(stats.skew(residuals)), 4),\n",
    "        \"kurtosis\": round(float(stats.kurtosis(residuals)), 4),\n",
    "    },\n",
    "    \"error_percentiles\": {str(p): round(float(v), 4) for p, v in zip(percentiles, pct_values)},\n",
    "    \"target_stats\": {\n",
    "        \"train_mean\": round(float(y_train.mean()), 4),\n",
    "        \"train_std\": round(float(y_train.std()), 4),\n",
    "        \"test_mean\": round(float(y_test.mean()), 4),\n",
    "        \"test_std\": round(float(y_test.std()), 4),\n",
    "    },\n",
    "}\n",
    "\n",
    "Path(metrics_json_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "with open(metrics_json_path, \"w\") as f:\n",
    "    json.dump(metrics_output, f, indent=2, default=str)\n",
    "\n",
    "print(f\"Metrics JSON saved → {metrics_json_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-32",
   "metadata": {},
   "source": [
    "## 8 — Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# CatBoost feature importance\n",
    "# ---------------------------------------------------------------------------\n",
    "if enable_feature_importance:\n",
    "    importances = final_model.get_feature_importance()\n",
    "    imp_df = (\n",
    "        pl.DataFrame({\"feature\": feature_names, \"importance\": importances})\n",
    "        .sort(\"importance\", descending=True)\n",
    "    )\n",
    "    top_n = min(20, len(imp_df))\n",
    "    top = imp_df.head(top_n)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, max(4, top_n * 0.35)))\n",
    "    ax.barh(top[\"feature\"].to_list()[::-1], top[\"importance\"].to_list()[::-1])\n",
    "    ax.set_xlabel(\"Importance\")\n",
    "    ax.set_title(f\"Top {top_n} Feature Importances\")\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(f\"{plots_dir}/feature_importance.png\", dpi=120)\n",
    "    plt.show()\n",
    "    print(f\"Saved → {plots_dir}/feature_importance.png\")\n",
    "else:\n",
    "    print(\"Feature importance disabled.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-34",
   "metadata": {},
   "source": [
    "## 9 — SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# SHAP explanations\n",
    "# ---------------------------------------------------------------------------\n",
    "if enable_shap:\n",
    "    import shap\n",
    "\n",
    "    n_sample = min(shap_sample_size, X_test.shape[0])\n",
    "    rng = np.random.RandomState(random_state)\n",
    "    idx = rng.choice(X_test.shape[0], size=n_sample, replace=False)\n",
    "    X_shap = X_test[idx]\n",
    "\n",
    "    explainer = shap.TreeExplainer(final_model)\n",
    "    shap_values = explainer.shap_values(X_shap)\n",
    "\n",
    "    # Summary bar plot\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    shap.summary_plot(\n",
    "        shap_values, X_shap,\n",
    "        feature_names=feature_names,\n",
    "        plot_type=\"bar\",\n",
    "        show=False,\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{plots_dir}/shap_bar.png\", dpi=120, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    print(f\"Saved → {plots_dir}/shap_bar.png\")\n",
    "\n",
    "    # Summary dot plot\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    shap.summary_plot(\n",
    "        shap_values, X_shap,\n",
    "        feature_names=feature_names,\n",
    "        show=False,\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{plots_dir}/shap_summary.png\", dpi=120, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    print(f\"Saved → {plots_dir}/shap_summary.png\")\n",
    "else:\n",
    "    print(\"SHAP disabled.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-36",
   "metadata": {},
   "source": [
    "## 10 — Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# Final summary\n",
    "# ---------------------------------------------------------------------------\n",
    "print(\"=\" * 60)\n",
    "print(\"RUN COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"  Model saved to:       {model_output_path}\")\n",
    "print(f\"  Metrics JSON:         {metrics_json_path}\")\n",
    "print(f\"  Plots directory:      {plots_dir}\")\n",
    "if executed_notebook_path:\n",
    "    print(f\"  Executed notebook:    {executed_notebook_path}\")\n",
    "print(f\"  RMSE:                 {rmse:,.4f}\")\n",
    "print(f\"  MAE:                  {mae:,.4f}\")\n",
    "print(f\"  R²:                   {r2:.4f}\")\n",
    "print(f\"  MAPE:                 {mape:.4f}\")\n",
    "print(f\"  Median AE:            {medae:,.4f}\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
